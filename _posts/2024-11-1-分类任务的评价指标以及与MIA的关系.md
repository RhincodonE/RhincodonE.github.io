---
layout: post
title: 分类任务的评价指标以及与MIA的关系
date: 2024-11-1
description: 本文主要介绍了分类任务的评价标准及优缺点和适用范围，以及MIA attack与各类指标的关系。
tags: metrics
categories: MIA
related_posts: false
---

# 机器学习模型评估指标详解

以下是机器学习模型常用评估指标的详细说明，包括**准确率**、**精确率**、**召回率**、**F1 值**、**AUC** 和 **G-mean** 的定义、优缺点。

---

### 1. 准确率 (Accuracy)
- **定义**：准确率是模型预测正确的样本占所有样本的比例。
  $$
  \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
  $$
  其中，TP 为真正例，TN 为真负例，FP 为假正例，FN 为假负例。
  
- **优点**：简单直观，适用于类别分布均衡的数据集。
- **缺点**：对于类别不平衡的数据集效果较差，因为少数类的错误可能被多数类的正确预测掩盖。

---

### 2. 精确率 (Precision)
- **定义**：精确率表示在预测为正类的样本中，真正为正类的比例。
  $$
  \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
  $$

- **优点**：在对误报容忍度较低的应用中（如癌症检测或欺诈检测）尤为重要。
- **缺点**：当负类样本非常多而正类样本很少时，精确率可能较高，导致对召回率的忽视。

---

### 3. 召回率 (Recall)
- **定义**：召回率表示所有实际为正类的样本中被正确预测为正类的比例。
  $$
  \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
  $$

- **优点**：在对漏报敏感的应用（如疾病检测）中效果显著。
- **缺点**：当模型倾向于将样本预测为正类时，召回率可能较高，但精确率会下降。

---

### 4. F1 值 (F1 Score)
- **定义**：F1 值是精确率和召回率的调和平均值，用于综合评估模型的精确度和召回度。
  $$
  \text{F1 Score} = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  $$

- **优点**：适用于类别不平衡的数据集，因为它权衡了精确率和召回率。
- **缺点**：不适用于只关注精确率或召回率的应用场景，因为它无法提供各个指标的详细信息。

---

### 5. AUC (Area Under Curve)
- **定义**：AUC 表示 ROC 曲线下的面积，ROC 曲线是通过绘制不同阈值下的 TPR（真正率）和 FPR（假正率）得到的曲线。
  $$
  \text{AUC} = \int \text{ROC Curve}
  $$

- **优点**：不受类别分布影响，能够直观衡量模型的分类能力。AUC 越接近 1，模型效果越好。
- **缺点**：对于类别不平衡严重的情况，AUC 可能掩盖少数类的错误分类问题。

---

### 6. G-Mean (几何平均)
- **定义**：G-mean 是模型在正类和负类上的分类效果的几何平均值，通常用于处理类别不平衡的数据。
  $$
  \text{G-Mean} = \sqrt{\text{Recall}_{\text{positive}} \times \text{Recall}_{\text{negative}}}
  $$

- **优点**：能够平衡模型在不同类别上的表现，适用于类别不平衡的情况。
- **缺点**：可能在极端不平衡数据上表现不佳，因为少数类的表现过于依赖召回率。

---

### 总结

| 指标       | 定义                                                                                              | 优点                                                   | 缺点                                                      |
|------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------|-----------------------------------------------------------|
| 准确率     | 所有正确预测的样本占所有样本的比例                                                               | 直观简单，适用于类别平衡的数据集                       | 类别不平衡时效果差                                         |
| 精确率     | 预测为正类的样本中真正为正类的比例                                                               | 适用于误报容忍度低的场景                               | 容易忽视召回率                                             |
| 召回率     | 实际为正类的样本中被正确预测为正类的比例                                                         | 适用于漏报敏感的场景                                   | 容易忽视精确率                                             |
| F1 值      | 精确率和召回率的调和平均值                                                                      | 平衡精确率和召回率，适用于类别不平衡的数据             | 无法提供精确率和召回率的详细信息                           |
| AUC        | ROC 曲线下面积，表示模型对不同阈值下的整体表现                                                 | 不受类别分布影响，能够直观衡量模型分类能力             | 严重不平衡数据时可能掩盖少数类错误                         |
| G-Mean     | 正类和负类召回率的几何平均值，适用于类别不平衡数据                                             | 平衡模型在不同类别上的表现                             | 在极端不平衡数据上表现可能不佳                             |

---

## TPR、FPR、TNR、FNR、L2 Error 和 L1 Error

### 1. TPR (True Positive Rate)
- **定义**：TPR（真正例率）是所有实际为正类的样本中被正确预测为正类的比例，等同于**召回率**。
  $$
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
  $$

### 2. FPR (False Positive Rate)
- **定义**：FPR（假正例率）是所有实际为负类的样本中被错误预测为正类的比例。
  $$
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
  $$

### 3. TNR (True Negative Rate)
- **定义**：TNR（真负例率）是所有实际为负类的样本中被正确预测为负类的比例，也称**特异性**。
  $$
  \text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}
  $$

### 4. FNR (False Negative Rate)
- **定义**：FNR（假负例率）是所有实际为正类的样本中被错误预测为负类的比例。
  $$
  \text{FNR} = \frac{\text{FN}}{\text{FN} + \text{TP}}
  $$

### 5. L2 Error (Mean Squared Error, MSE)
- **定义**：L2误差是预测值与实际值之间差值的平方和的均值。
  $$
  \text{L2 Error} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
  $$

### 6. L1 Error (Mean Absolute Error, MAE)
- **定义**：L1误差是预测值与实际值之间差值的绝对值的均值。
  $$
  \text{L1 Error} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
  $$

---

## 为什么在 MIA 中更关注低 FPR 下的 TPR？

在**成员推断攻击**（Membership Inference Attack, MIA）中，更关注在低 FPR（False Positive Rate）下的 TPR（True Positive Rate）。原因包括：

1. **隐私风险**：低 FPR 能降低误判非成员为成员的可能性。
2. **误报影响**：高 FPR 会导致大量误报，掩盖攻击模型的真实效果。
3. **攻击强度**：在极少误报情况下，高 TPR 表示攻击模型在严格条件下的攻击强度。
4. **防御敏感性**：低 FPR 下的 TPR 能更好地评估防御机制的有效性。
5. **实际需求**：在金融或医疗领域，低 FPR 的高 TPR 更符合实际隐私需求。

低 FPR 下的 TPR 能帮助揭示模型在严格隐私保护条件下的风险。

