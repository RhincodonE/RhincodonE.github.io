<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rhincodone.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rhincodone.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-01T16:16:10+00:00</updated><id>https://rhincodone.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">分类任务的评价指标以及与MIA的关系</title><link href="https://rhincodone.github.io/posts/2024-11-1-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E4%BB%A5%E5%8F%8A%E4%B8%8EMIA%E7%9A%84%E5%85%B3%E7%B3%BB/" rel="alternate" type="text/html" title="分类任务的评价指标以及与MIA的关系"/><published>2024-11-01T00:00:00+00:00</published><updated>2024-11-01T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E4%BB%A5%E5%8F%8A%E4%B8%8EMIA%E7%9A%84%E5%85%B3%E7%B3%BB</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-11-1-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E4%BB%A5%E5%8F%8A%E4%B8%8EMIA%E7%9A%84%E5%85%B3%E7%B3%BB/"><![CDATA[<h1 id="机器学习模型评估指标详解">机器学习模型评估指标详解</h1> <p>以下是机器学习模型常用评估指标的详细说明，包括<strong>准确率</strong>、<strong>精确率</strong>、<strong>召回率</strong>、<strong>F1 值</strong>、<strong>AUC</strong> 和 <strong>G-mean</strong> 的定义、优缺点。</p> <hr/> <h3 id="1-准确率-accuracy">1. 准确率 (Accuracy)</h3> <ul> <li> <p><strong>定义</strong>：准确率是模型预测正确的样本占所有样本的比例。 \(\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\) 其中，TP 为真正例，TN 为真负例，FP 为假正例，FN 为假负例。</p> </li> <li><strong>优点</strong>：简单直观，适用于类别分布均衡的数据集。</li> <li><strong>缺点</strong>：对于类别不平衡的数据集效果较差，因为少数类的错误可能被多数类的正确预测掩盖。</li> </ul> <hr/> <h3 id="2-精确率-precision">2. 精确率 (Precision)</h3> <ul> <li> <p><strong>定义</strong>：精确率表示在预测为正类的样本中，真正为正类的比例。 \(\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\)</p> </li> <li><strong>优点</strong>：在对误报容忍度较低的应用中（如癌症检测或欺诈检测）尤为重要。</li> <li><strong>缺点</strong>：当负类样本非常多而正类样本很少时，精确率可能较高，导致对召回率的忽视。</li> </ul> <hr/> <h3 id="3-召回率-recall">3. 召回率 (Recall)</h3> <ul> <li> <p><strong>定义</strong>：召回率表示所有实际为正类的样本中被正确预测为正类的比例。 \(\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\)</p> </li> <li><strong>优点</strong>：在对漏报敏感的应用（如疾病检测）中效果显著。</li> <li><strong>缺点</strong>：当模型倾向于将样本预测为正类时，召回率可能较高，但精确率会下降。</li> </ul> <hr/> <h3 id="4-f1-值-f1-score">4. F1 值 (F1 Score)</h3> <ul> <li> <p><strong>定义</strong>：F1 值是精确率和召回率的调和平均值，用于综合评估模型的精确度和召回度。 \(\text{F1 Score} = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\)</p> </li> <li><strong>优点</strong>：适用于类别不平衡的数据集，因为它权衡了精确率和召回率。</li> <li><strong>缺点</strong>：不适用于只关注精确率或召回率的应用场景，因为它无法提供各个指标的详细信息。</li> </ul> <hr/> <h3 id="5-auc-area-under-curve">5. AUC (Area Under Curve)</h3> <ul> <li> <p><strong>定义</strong>：AUC 表示 ROC 曲线下的面积，ROC 曲线是通过绘制不同阈值下的 TPR（真正率）和 FPR（假正率）得到的曲线。 \(\text{AUC} = \int \text{ROC Curve}\)</p> </li> <li><strong>优点</strong>：不受类别分布影响，能够直观衡量模型的分类能力。AUC 越接近 1，模型效果越好。</li> <li><strong>缺点</strong>：对于类别不平衡严重的情况，AUC 可能掩盖少数类的错误分类问题。</li> </ul> <hr/> <h3 id="6-g-mean-几何平均">6. G-Mean (几何平均)</h3> <ul> <li> <p><strong>定义</strong>：G-mean 是模型在正类和负类上的分类效果的几何平均值，通常用于处理类别不平衡的数据。 \(\text{G-Mean} = \sqrt{\text{Recall}_{\text{positive}} \times \text{Recall}_{\text{negative}}}\)</p> </li> <li><strong>优点</strong>：能够平衡模型在不同类别上的表现，适用于类别不平衡的情况。</li> <li><strong>缺点</strong>：可能在极端不平衡数据上表现不佳，因为少数类的表现过于依赖召回率。</li> </ul> <hr/> <h3 id="总结">总结</h3> <table> <thead> <tr> <th>指标</th> <th>定义</th> <th>优点</th> <th>缺点</th> </tr> </thead> <tbody> <tr> <td>准确率</td> <td>所有正确预测的样本占所有样本的比例</td> <td>直观简单，适用于类别平衡的数据集</td> <td>类别不平衡时效果差</td> </tr> <tr> <td>精确率</td> <td>预测为正类的样本中真正为正类的比例</td> <td>适用于误报容忍度低的场景</td> <td>容易忽视召回率</td> </tr> <tr> <td>召回率</td> <td>实际为正类的样本中被正确预测为正类的比例</td> <td>适用于漏报敏感的场景</td> <td>容易忽视精确率</td> </tr> <tr> <td>F1 值</td> <td>精确率和召回率的调和平均值</td> <td>平衡精确率和召回率，适用于类别不平衡的数据</td> <td>无法提供精确率和召回率的详细信息</td> </tr> <tr> <td>AUC</td> <td>ROC 曲线下面积，表示模型对不同阈值下的整体表现</td> <td>不受类别分布影响，能够直观衡量模型分类能力</td> <td>严重不平衡数据时可能掩盖少数类错误</td> </tr> <tr> <td>G-Mean</td> <td>正类和负类召回率的几何平均值，适用于类别不平衡数据</td> <td>平衡模型在不同类别上的表现</td> <td>在极端不平衡数据上表现可能不佳</td> </tr> </tbody> </table> <hr/> <h2 id="tprfprtnrfnrl2-error-和-l1-error">TPR、FPR、TNR、FNR、L2 Error 和 L1 Error</h2> <h3 id="1-tpr-true-positive-rate">1. TPR (True Positive Rate)</h3> <ul> <li><strong>定义</strong>：TPR（真正例率）是所有实际为正类的样本中被正确预测为正类的比例，等同于<strong>召回率</strong>。 \(\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\)</li> </ul> <h3 id="2-fpr-false-positive-rate">2. FPR (False Positive Rate)</h3> <ul> <li><strong>定义</strong>：FPR（假正例率）是所有实际为负类的样本中被错误预测为正类的比例。 \(\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\)</li> </ul> <h3 id="3-tnr-true-negative-rate">3. TNR (True Negative Rate)</h3> <ul> <li><strong>定义</strong>：TNR（真负例率）是所有实际为负类的样本中被正确预测为负类的比例，也称<strong>特异性</strong>。 \(\text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\)</li> </ul> <h3 id="4-fnr-false-negative-rate">4. FNR (False Negative Rate)</h3> <ul> <li><strong>定义</strong>：FNR（假负例率）是所有实际为正类的样本中被错误预测为负类的比例。 \(\text{FNR} = \frac{\text{FN}}{\text{FN} + \text{TP}}\)</li> </ul> <h3 id="5-l2-error-mean-squared-error-mse">5. L2 Error (Mean Squared Error, MSE)</h3> <ul> <li><strong>定义</strong>：L2误差是预测值与实际值之间差值的平方和的均值。 \(\text{L2 Error} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\)</li> </ul> <h3 id="6-l1-error-mean-absolute-error-mae">6. L1 Error (Mean Absolute Error, MAE)</h3> <ul> <li><strong>定义</strong>：L1误差是预测值与实际值之间差值的绝对值的均值。 \(\text{L1 Error} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\)</li> </ul> <hr/> <h2 id="为什么在-mia-中更关注低-fpr-下的-tpr">为什么在 MIA 中更关注低 FPR 下的 TPR？</h2> <p>在<strong>成员推断攻击</strong>（Membership Inference Attack, MIA）中，更关注在低 FPR（False Positive Rate）下的 TPR（True Positive Rate）。原因包括：</p> <ol> <li><strong>隐私风险</strong>：低 FPR 能降低误判非成员为成员的可能性。</li> <li><strong>误报影响</strong>：高 FPR 会导致大量误报，掩盖攻击模型的真实效果。</li> <li><strong>攻击强度</strong>：在极少误报情况下，高 TPR 表示攻击模型在严格条件下的攻击强度。</li> <li><strong>防御敏感性</strong>：低 FPR 下的 TPR 能更好地评估防御机制的有效性。</li> <li><strong>实际需求</strong>：在金融或医疗领域，低 FPR 的高 TPR 更符合实际隐私需求。</li> </ol> <p>低 FPR 下的 TPR 能帮助揭示模型在严格隐私保护条件下的风险。</p>]]></content><author><name></name></author><category term="MIA"/><category term="metrics"/><summary type="html"><![CDATA[本文主要介绍了分类任务的评价标准及优缺点和适用范围，以及MIA attack与各类指标的关系。]]></summary></entry><entry><title type="html">Membership Inferernce Attacks on LLM (Large Language Models)</title><link href="https://rhincodone.github.io/posts/2024-03-23-Machine-Unlearning-On-LLM/" rel="alternate" type="text/html" title="Membership Inferernce Attacks on LLM (Large Language Models)"/><published>2024-03-23T00:00:00+00:00</published><updated>2024-03-23T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/Machine-Unlearning-On-LLM</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-03-23-Machine-Unlearning-On-LLM/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Membership Inference Attacks (MIAs) are designed to determine if a specific data record was used in a model’s training set. Introduced by Shokri et al. in 2017[1], MIAs have since been recognized for their relevance to privacy concerns within machine learning. This introduction will cover the fundamental concept of MIAs and their initial application. Following that, we’ll explore the adaptation of MIAs to large-scale Large Language Models (LLMs) and identify the challenges that diminish MIAs’ effectiveness in this context. To conclude, the discussion will shift to prospective developments and the role MIAs may play in enhancing the privacy of LLMs.</p> <h2 id="membership-inference-attacks">Membership inference attacks</h2> <p>The intuition behind MIAs lies in overfitting of the model to the training data. Overfitting is a common challenge in machine learning, where a model learns too much from its training data, capturing excessive detail that hinders its ability to generalize to unseen data. This occurs when a model, after extensive training, performs exceptionally well on the training data (for instance, achieving 100% accuracy) but poorly on a testing dataset, effectively making random guesses. The root of this issue lies in the model memorizing specific details of the training images—such as the exact pixel positions—rather than learning the broader, general features necessary to distinguish between different individuals’ faces. Although overfitting cannot be completely eliminated due to the model’s inherent limitation of learning solely from its training dataset, various strategies can mitigate its effects.</p> <p>This concept of overfitting also underpins the differential performance of models on training versus non-training samples, evident through their loss function outputs. For example, a model might show lower loss (e.g., smaller cross-entropy) for training samples than for testing samples. This discrepancy is exploited in membership inference attacks, which aim to determine whether a specific sample was part of the model’s training set based on the model’s response to that sample.</p> <p>In early implementations of membership inference attacks, the approach involves splitting an original dataset into two parts: one for members (data included in the training set) and one for non-members (data excluded from the training set). The “member” data is used to train a target model, resulting in a model trained specifically on that subset. Subsequently, samples from both the member and non-member datasets are input into the trained model, which outputs a probability vector for each sample if the model is of the classification type. These vectors, along with labels indicating “member” or “non-member” status based on the sample’s origin, are combined with the sample’s true classification label to form a new dataset. This new dataset, referred to as the attack dataset, includes each sample’s probability vector, its true class label, and its membership label. An attack model—a binary classification model—is then trained on this attack dataset to discern whether a given sample was part of the model’s training set.</p> <h2 id="mia-on-llm">MIA on LLM</h2> <p>Although MIA has been deeply investigated in small-scale networks, in the era of large languge model (LLM), it is not studied on the LLM enough. Most of the works directly adopt the intuition of MIA-classify the members and non-members based on some indicators (The implementations of the attacks can be found at [6]):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MIALLM-480.webp 480w,/assets/img/MIALLM-800.webp 800w,/assets/img/MIALLM-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/MIALLM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p><strong>LOSS [2]:</strong> Considers the model’s computed loss over the target sample. \begin{equation}f(x;M) = L(x;M)\end{equation}</p> </li> <li> <p><strong>Reference-based [3]:</strong> Attempts to improve the precision of the LOSS attack and reduce the false negative rate by accounting for the intrinsic complexity of the target point \(x\) by calibrating \(L(x;M)\), with respect to another reference model \((M_{ref})\), which is trained on data from the same distribution as \(D\), but not necessarily the same data. \begin{equation}f(x;M) = L(x;M) - L(x;M_{ref})\end{equation}</p> </li> <li> <p><strong>Zlib Entropy [3]:</strong> Calibrates the sample’s loss under the target model using the sample’s zlib compression size. \begin{equation}f(x;M) = \frac{L(x;M)}{zlib(x)}\end{equation}</p> </li> <li> <p><strong>Min-k% Prob [4]:</strong> Uses the \(k\%\) of tokens with the lowest likelihoods to compute a score instead of averaging over all token probabilities as in loss. \begin{equation}f(x;M) = \frac{1}{|min-k(x)|} \sum_{x_i \in min-k(x)} -\log(p(x_i | x_1, …, x_{i-1}))\end{equation}</p> </li> <li> <p><strong>Neighborhood Attack [5]:</strong> Uses an estimate of the curvature of the loss function at a given sample, computed by perturbing the target sequence to create \(n\) ‘neighboring’ points, and comparing the loss of the target \(x\), with its neighbors \(\tilde{x}\). \begin{equation}f(x;M) = L(x;M) - \frac{1}{n} \sum_{i=1}^{n} L(\tilde{x}_i;M)\end{equation}</p> </li> </ol> <h2 id="why-mia-cares">Why MIA cares</h2> <p>Membership Inference Attacks (MIA) are significant for privacy because they can expose whether an individual’s data was used in training a machine learning model. This might seem technical but has real-world privacy implications, especially in sensitive contexts. For a simple example, consider a machine learning model trained on health records to predict disease outcomes. If an attacker can use MIA to determine that a particular individual’s data was used in the training set, they might indirectly learn something about that individual’s health status, even if the data was supposed to be anonymous.</p> <p>Imagine a hospital that develops a model to predict diabetes risk based on patient records. If an attacker can apply MIA to this model and discover that John Doe’s data was used in training, they could infer that John might have been at risk of or diagnosed with diabetes, information that’s supposed to be private. This scenario illustrates why MIA is a concern for privacy: it can lead to unintended disclosures of personal information, undermining the anonymity and confidentiality of sensitive data.</p> <h2 id="problems-in-mia">Problems in MIA</h2> <p>As research into Membership Inference Attacks (MIA) progresses, several complexities and challenges have emerged. A key question is the interpretation of the probabilities produced by MIA models. For instance, if an MIA model assigns a 60% probability to a sample being a “member” (i.e., part of the training data), the implications for privacy remain unclear. This uncertainty extends to minor modifications of the sample, such as altering a single pixel—does this still warrant a “member” classification, and what does that mean for privacy?</p> <p>Carlini et al. have highlighted that MIA tends to be more effective at identifying “non-member” records rather than “members.”[7] This observation suggests that the privacy risks associated with membership inference might not be as significant for member samples as previously thought.</p> <p>Furthermore, the nuances of Large Language Models (LLMs) and their susceptibility to MIAs warrant further exploration. Specific characteristics of LLMs, such as their capacity for data memorization, play a crucial role in how they might compromise the anonymity of training data. However, the full extent of these features’ impact on training data membership remains partially understood.</p> <p>Given these issues, the future and practical relevance of MIA are subjects of ongoing debate. As we continue to unravel the complexities of MIAs and their effects on privacy, it is crucial to refine our understanding of these attacks and their implications for the security of machine learning models.</p> <h2 id="potential-of-mia">Potential of MIA</h2> <p>Recent research highlights the potential of Membership Inference Attacks (MIA) for auditing the privacy of algorithms designed for differential privacy. Differential privacy provides a solid guarantee of privacy, leading to the proposal of differentially private machine learning algorithms, such as DP-SGD and DP-Adam. Users set a theoretical privacy budget, \(\epsilon_{theory}\), and a relaxation parameter, \(\delta\), aiming for the trained model to achieve \((\epsilon_{theory},\delta)\)-differential privacy. However, assessing a model’s actual privacy level has been challenging, as differential privacy offers a worst-case guarantee, suggesting that the practical privacy level might be more stringent (practical \(\epsilon\) is smaller than \(\epsilon_{theory}\)).</p> <p>Studies now indicate that MIA can estimate the lower bound of the practical \(\epsilon\), denoted as \(\epsilon_{LB}\). This suggests the actual \(\epsilon\) for a model’s privacy lies within the range \([\epsilon_{LB},\epsilon_{theory}]\), providing a clearer picture of its privacy assurances. This insight underscores MIA’s role in evaluating and ensuring the privacy of differentially private machine learning algorithms[8][9][10].</p> <p>Several differentially private fine-tuning techniques have been developed for Large Language Models (LLMs) to secure their differential privacy [11][12]. Despite these advancements, there remains a lack of research into the auditing of these models’ actual privacy levels. Therefore, a valuable direction for future work could be to employ Membership Inference Attacks (MIAs) to assess and audit the privacy of LLMs that have undergone differential privacy fine-tuning. This approach could provide a clearer understanding of how private the LLMs really are, beyond theoretical guarantees.</p> <h2 id="references">References</h2> <p>[1] Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017, May). Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP) (pp. 3-18). IEEE.</p> <p>[2] Yeom, S., Giacomelli, I., Fredrikson, M., &amp; Jha, S. (2018, July). Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF) (pp. 268-282). IEEE.</p> <p>[3] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … &amp; Raffel, C. (2021). Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) (pp. 2633-2650).</p> <p>[4] Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., … &amp; Zettlemoyer, L. (2023). Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789.</p> <p>[5] Mattern, J., Mireshghallah, F., Jin, Z., Schölkopf, B., Sachan, M., &amp; Berg-Kirkpatrick, T. (2023). Membership inference attacks against language models via neighbourhood comparison. arXiv preprint arXiv:2305.18462.</p> <p>[6] https://github.com/iamgroot42/mimir</p> <p>[7] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis and F. Tramèr, “Membership Inference Attacks From First Principles,” 2022 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 2022, pp. 1897-1914, doi: 10.1109/SP46214.2022.9833649.</p> <p>[8] Tramer, F., Terzis, A., Steinke, T., Song, S., Jagielski, M., &amp; Carlini, N. (2022). Debugging differential privacy: A case study for privacy auditing. arXiv preprint arXiv:2202.12219.</p> <p>[9] Nasr, M., Hayes, J., Steinke, T., Balle, B., Tramèr, F., Jagielski, M., … &amp; Terzis, A. (2023). Tight auditing of differentially private machine learning. In 32nd USENIX Security Symposium (USENIX Security 23) (pp. 1631-1648).</p> <p>[10] Steinke, T., Nasr, M., &amp; Jagielski, M. (2024). Privacy auditing with one (1) training run. Advances in Neural Information Processing Systems, 36.</p> <p>[11] Behnia, R., Ebrahimi, M. R., Pacheco, J., &amp; Padmanabhan, B. (2022, November). Ew-tune: A framework for privately fine-tuning large language models with differential privacy. In 2022 IEEE International Conference on Data Mining Workshops (ICDMW) (pp. 560-566). IEEE.</p> <p>[12] Singh, T., Aditya, H., Madisetti, V. K., &amp; Bahga, A. (2024). Whispered Tuning: Data Privacy Preservation in Fine-Tuning LLMs through Differential Privacy. Journal of Software Engineering and Applications, 17(1), 1-22.</p>]]></content><author><name></name></author><category term="Machine"/><category term="Unlearning"/><category term="MU"/><summary type="html"><![CDATA[In this post, I'll introduce what is membership inference attack, how it's used on LLM, and what's the potential problem on it.]]></summary></entry><entry><title type="html">A Foundation of Differential Privacy</title><link href="https://rhincodone.github.io/posts/2024-03-05-Differential-Privacy/" rel="alternate" type="text/html" title="A Foundation of Differential Privacy"/><published>2024-03-05T00:00:00+00:00</published><updated>2024-03-05T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/Differential-Privacy</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-03-05-Differential-Privacy/"><![CDATA[<h2 id="introduction-with-an-example">Introduction with an example</h2> <p>Differential privacy (DP) is a privacy-preserving concept in data analysis and statistics. The goal of differential privacy is to allow the inclusion of an individual’s data in a dataset for analysis while protecting the privacy of that individual.</p> <h2 id="example">Example</h2> <p>So, how does DP achieve this? Let’s take a simple example:</p> <p>Suppose you are part of a company that wants to calculate the average salary of its employees without revealing the salary of any specific employee. The company collects the following data:</p> <p><em>Employee A: $60,000</em> <em>Employee B: $70,000</em> <em>Employee C: $80,000</em> <em>Employee D: $90,000</em></p> <p><strong>Non-Differentially Private Calculation:</strong></p> <p>In a traditional average calculation, you might sum all salaries and divide by the number of employees: \begin{equation} \text{Average} = \frac{60,000 + 70,000 + 80,000 + 90,000}{4} = 75,000 \end{equation}</p> <p><strong>Differentially Private Calculation:</strong></p> <p>In a differentially private approach, you add random noise to the calculation to protect individual privacy. Let’s say you add random noise between -1,000 and +1,000 to each employee’s salary:</p> <p>\begin{equation} \text{Average} = \frac{60,000 + 70,000 + 80,000 + 90,000 + \text{noise}}{4} \end{equation}</p> <p>Here, the noise is a random value chosen from the range -1,000 to +1,000. This ensures that even if an individual’s salary changes slightly, it is challenging to determine which specific employee’s salary contributed to the final result.</p> <p>In essence, differential privacy aims to introduce uncertainty in the output distribution. Considering two datasets that differ only in one data record, a differentially private algorithm will generate output distributions that are slightly different for these two datasets. In other words, observers cannot discern which dataset the algorithm used from the output, and thus, they cannot determine whether a specific data record is in the dataset or not.</p>]]></content><author><name></name></author><category term="Differential"/><category term="privacy"/><category term="DP"/><summary type="html"><![CDATA[In this post, I'll generally introduce what is differential privacy with an example]]></summary></entry></feed>