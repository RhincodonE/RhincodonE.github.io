<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rhincodone.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rhincodone.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-15T14:02:37+00:00</updated><id>https://rhincodone.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Fine-Tuning Vision Transformer (ViT) on Tiny ImageNet Dataset</title><link href="https://rhincodone.github.io/posts/2024-11-15-Fine_Tuning_ViT_Tiny_ImageNet/" rel="alternate" type="text/html" title="Fine-Tuning Vision Transformer (ViT) on Tiny ImageNet Dataset"/><published>2024-11-15T00:00:00+00:00</published><updated>2024-11-15T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/Fine_Tuning_ViT_Tiny_ImageNet</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-11-15-Fine_Tuning_ViT_Tiny_ImageNet/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This document provides a detailed overview of the strategy employed to fine-tune a Vision Transformer (ViT) on the Tiny ImageNet dataset, achieving a validation accuracy of <strong>90.5% within 10 epochs</strong>.</p> <h2 id="dataset-description">Dataset Description</h2> <ul> <li><strong>Dataset</strong>: Tiny ImageNet</li> <li><strong>Number of Classes</strong>: 200</li> <li><strong>Image Size</strong>: 64x64 resized to 384x384 for ViT</li> </ul> <h2 id="model-configuration">Model Configuration</h2> <ul> <li><strong>Model</strong>: ViT-Base with patch size 16 (<code class="language-plaintext highlighter-rouge">vit_base_patch16_384</code>)</li> <li><strong>Pretrained Weights</strong>: Used pretrained weights from ImageNet</li> <li><strong>Optimizer</strong>: SGD with momentum (0.9)</li> <li><strong>Learning Rate</strong>: 1e-4</li> <li><strong>Weight Decay</strong>: 0.01</li> <li><strong>Scheduler</strong>: Cosine Annealing Learning Rate</li> <li><strong>Loss Function</strong>: Soft Target Cross-Entropy (for Mixup/CutMix)</li> <li><strong>Augmentation</strong>: RandAugment, Random Erasing, Mixup, and CutMix</li> </ul> <h2 id="strategy">Strategy</h2> <h3 id="data-preprocessing">Data Preprocessing</h3> <ol> <li><strong>Image Resizing</strong>: <ul> <li>Images were resized to 384x384 to match the input dimensions required by the Vision Transformer (ViT) model. This ensures that the patching mechanism of the ViT (16x16 patches in this case) works seamlessly, dividing the images into the correct number of patches for transformer-based processing.</li> </ul> </li> <li><strong>Enhanced Data Augmentations</strong>: <ul> <li><strong>RandAugment</strong>: <ul> <li>Method: This augmentation policy applies a random combination of transformations such as rotation, brightness adjustment, and flipping, chosen from a predefined pool of operations.</li> <li>Implementation: Integrated using the <code class="language-plaintext highlighter-rouge">RandAugment</code> class from <code class="language-plaintext highlighter-rouge">torchvision.transforms</code>.</li> <li>Intuition: Augmentations simulate diverse scenarios in the dataset, enhancing model robustness to unseen variations in real-world applications.</li> </ul> </li> <li><strong>Random Erasing</strong>: <ul> <li>Method: Randomly erases parts of an image during training by replacing selected regions with random pixel values.</li> <li>Probability: Set to 0.25, meaning 25% of training images had a random region erased.</li> <li>Intuition: Prevents the model from over-relying on specific regions of an image, encouraging it to learn more generalized features.</li> </ul> </li> </ul> </li> </ol> <h3 id="training-enhancements">Training Enhancements</h3> <ol> <li><strong>Mixup and CutMix</strong>: <ul> <li><strong>Mixup</strong>: <ul> <li>Method: Mixup blends two training examples and their labels, creating a synthetic training sample:<br/> [ \tilde{x} = \lambda x_i + (1 - \lambda) x_j, \quad \tilde{y} = \lambda y_i + (1 - \lambda) y_j ]<br/> where ( \lambda ) is sampled from a Beta distribution.</li> <li>Implementation: Integrated using the <code class="language-plaintext highlighter-rouge">Mixup</code> utility from the <code class="language-plaintext highlighter-rouge">timm</code> library.</li> <li>Intuition: Mixup smoothens decision boundaries and reduces overfitting, as the model cannot rely on “hard” training labels.</li> </ul> </li> <li><strong>CutMix</strong>: <ul> <li>Method: Similar to Mixup, but instead of blending the entire images, rectangular patches of one image replace patches in another. Labels are proportionally adjusted.</li> <li>Implementation: Configured with probabilities for blending and patch placement using <code class="language-plaintext highlighter-rouge">timm.data.Mixup</code>.</li> <li>Intuition: Encourages spatially aware feature learning, improving robustness to occlusions or corruptions.</li> </ul> </li> </ul> </li> <li><strong>Stochastic Depth</strong>: <ul> <li>Method: During training, randomly drops a subset of transformer blocks in each forward pass, controlled by a drop probability.</li> <li>Implementation: Applied a drop probability of 0.1 to regularize deeper layers using <code class="language-plaintext highlighter-rouge">timm.layers.DropPath</code>.</li> <li>Intuition: Mimics an ensemble effect by allowing the model to explore multiple sub-networks, reducing overfitting and improving generalization.</li> </ul> </li> <li><strong>AMP (Automatic Mixed Precision)</strong>: <ul> <li>Method: Combines half-precision and full-precision computations dynamically during training.</li> <li>Implementation: Enabled with <code class="language-plaintext highlighter-rouge">torch.amp.GradScaler</code> and <code class="language-plaintext highlighter-rouge">torch.cuda.amp.autocast</code>.</li> <li>Intuition: Reduces GPU memory usage and accelerates training while maintaining model performance, especially useful for computationally intensive ViT models.</li> </ul> </li> </ol> <h3 id="training-loop">Training Loop</h3> <ul> <li><strong>Epochs</strong>: Trained for up to 50 epochs but utilized early stopping after achieving peak validation accuracy (90.5%) at 10 epochs.</li> <li><strong>Batch Size</strong>: Set to 128, optimized for GPU memory utilization.</li> <li><strong>Logging</strong>: Metrics, including training and validation loss and accuracy, were logged using TensorBoard. Logging frequency was every 100 batches to balance granularity and performance overhead.</li> </ul> <h3 id="validation">Validation</h3> <ul> <li>Standard Cross-Entropy loss was used during validation for hard-label accuracy computation. Unlike the training phase, which used soft-label losses (Mixup and CutMix), validation focused purely on the model’s ability to classify with confidence in real-world scenarios.</li> </ul> <hr/> <h3 id="layer-fine-tuning-strategy">Layer Fine-Tuning Strategy</h3> <p>The experiment tested two configurations for fine-tuning:</p> <ol> <li><strong>Fine-Tuning All Layers</strong>: <ul> <li>In this setting, all layers of the ViT model were unfrozen, allowing gradient updates to modify the pretrained weights.</li> <li><strong>Result</strong>: Achieved a validation accuracy of 90.5%, demonstrating the ability of the model to adapt its internal representations to the Tiny ImageNet dataset.</li> </ul> </li> <li><strong>Fine-Tuning the Last Fully Connected Layer Only</strong>: <ul> <li>In this setting, only the final classification head (Fully Connected Layer) was updated, while all transformer layers were frozen.</li> <li><strong>Result</strong>: Achieved a validation accuracy of 72.3%, indicating limited capacity to adapt the learned features to the new dataset.</li> </ul> </li> </ol> <p><strong>Analysis</strong>:</p> <ul> <li><strong>Why Fine-Tuning All Layers Performed Better</strong>: <ul> <li>The pretrained ViT model was trained on ImageNet, which shares some similarities with Tiny ImageNet but differs in scale and distribution.</li> <li>Fine-tuning all layers allowed the model to adjust its intermediate representations to the specific features and patterns of the Tiny ImageNet dataset, leading to significantly better performance.</li> </ul> </li> <li><strong>When to Fine-Tune Specific Layers</strong>: <ul> <li>Fine-tuning specific layers, such as only the classification head, may suffice for tasks with highly similar datasets (e.g., same domain). However, for diverse datasets, fine-tuning more or all layers is generally necessary.</li> </ul> </li> </ul> <hr/> <p><strong>Key Takeaway</strong>: Fine-tuning the entire network maximized the model’s adaptability to Tiny ImageNet, yielding superior performance. However, this comes at a higher computational cost compared to only tuning the last layer.</p> <h2 id="results">Results</h2> <ul> <li><strong>Validation Accuracy</strong>: 90.5% after 10 epochs</li> <li><strong>Training Time</strong>: Approximately 30 minutes per epoch on a single GPU</li> <li><strong>Best Model Saved</strong>: Model checkpoint saved at <code class="language-plaintext highlighter-rouge">./models/best_vit_tiny_imagenet.pth</code></li> </ul> <h2 id="key-insights">Key Insights</h2> <ol> <li><strong>Enhanced Augmentations</strong>: The combination of RandAugment, Mixup, and CutMix improved generalization.</li> <li><strong>Cosine Annealing</strong>: Helped achieve smooth convergence with the learning rate.</li> <li><strong>Pretrained Weights</strong>: Accelerated convergence and boosted performance significantly.</li> </ol> <hr/> <p><strong>Repository Setup</strong>: The code for this implementation, including the preprocessing and training pipeline, is structured for easy reproducibility. Ensure you have the following dependencies installed:</p> <ul> <li>PyTorch</li> <li>torchvision</li> <li>timm</li> <li>tqdm</li> </ul> <h2 id="code">Code</h2> <p>```python import os import shutil import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from timm import create_model from torch.optim.lr_scheduler import CosineAnnealingLR from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter from tqdm import tqdm # For progress bar os.environ[‘HF_HOME’] = ‘/tmp/ygu2/hf_cache_custom’ os.environ[‘HUGGINGFACE_HUB_CACHE’] = ‘/tmp/ygu2/hf_cache_custom/hub’</p> <h1 id="import-mixup-and-cutmix-utilities-from-timm">Import Mixup and CutMix utilities from timm</h1> <p>from timm.data import Mixup from timm.loss import SoftTargetCrossEntropy</p> <h1 id="optional-import-randaugment-for-enhanced-data-augmentation">Optional: Import RandAugment for enhanced data augmentation</h1> <p>from torchvision.transforms import RandAugment</p> <h1 id="set-cudnn-benchmark-for-optimized-performance">Set CuDNN Benchmark for optimized performance</h1> <p>torch.backends.cudnn.benchmark = True</p> <h1 id="paths-and-constants">Paths and Constants</h1> <p>data_dir = “./datasets/tiny-imagenet-200” num_classes = 200 batch_size = 128 # Adjust based on GPU memory num_epochs = 50 # Increased number of epochs for better convergence learning_rate = 1e-4 # Lowered learning rate for fine-tuning weight_decay = 0.01 # Adjusted weight decay image_size = 384 log_interval = 100 # Log metrics every 100 batches</p> <h1 id="reorganize-validation-data">Reorganize validation data</h1> <p>val_dir = os.path.join(data_dir, ‘val’) val_images_dir = os.path.join(val_dir, ‘images’) val_annotations_file = os.path.join(val_dir, ‘val_annotations.txt’)</p> <h1 id="create-a-mapping-from-image-filenames-to-their-labels">Create a mapping from image filenames to their labels</h1> <p>val_img_dict = {} with open(val_annotations_file, ‘r’) as f: for line in f.readlines(): words = line.strip().split(‘\t’) val_img_dict[words[0]] = words[1]</p> <h1 id="create-directories-for-each-class-if-they-dont-exist">Create directories for each class if they don’t exist</h1> <p>for label in set(val_img_dict.values()): label_dir = os.path.join(val_images_dir, label) if not os.path.exists(label_dir): os.makedirs(label_dir)</p> <h1 id="move-images-into-the-corresponding-label-directories">Move images into the corresponding label directories</h1> <p>for img_filename, label in val_img_dict.items(): src = os.path.join(val_images_dir, img_filename) dst = os.path.join(val_images_dir, label, img_filename) if os.path.exists(src): shutil.move(src, dst)</p> <h1 id="data-augmentation-and-transformations">Data Augmentation and Transformations</h1> <p>transform_train = transforms.Compose([ transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC), transforms.RandomHorizontalFlip(), RandAugment(), # Enhanced augmentation transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), transforms.RandomErasing(p=0.25), ])</p> <p>transform_test = transforms.Compose([ transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), ])</p> <h1 id="load-datasets">Load Datasets</h1> <p>train_dataset = datasets.ImageFolder(os.path.join(data_dir, ‘train’), transform=transform_train) train_loader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, # Reduced from 8 to 2 pin_memory=True, prefetch_factor=4, persistent_workers=True )</p> <p>val_dataset = datasets.ImageFolder(val_images_dir, transform=transform_test) val_loader = DataLoader( val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, # Reduced from 8 to 2 pin_memory=True, prefetch_factor=4, persistent_workers=True )</p> <h1 id="create-vision-transformer-vit-model">Create Vision Transformer (ViT) Model</h1> <p>model = create_model(‘vit_base_patch16_384’, pretrained=True, num_classes=num_classes)</p> <h1 id="apply-stochastic-depth">Apply Stochastic Depth</h1> <p>from timm.layers import DropPath # Updated import path</p> <p>def apply_stochastic_depth(model, drop_prob): for module in model.modules(): if isinstance(module, DropPath): module.drop_prob = drop_prob</p> <p>apply_stochastic_depth(model, drop_prob=0.1)</p> <h1 id="unfreeze-the-entire-model-for-fine-tuning">Unfreeze the entire model for fine-tuning</h1> <p>for param in model.parameters(): param.requires_grad = True</p> <h1 id="use-dataparallel-for-multiple-gpus-if-available">Use DataParallel for multiple GPUs if available</h1> <p>device = torch.device(“cuda” if torch.cuda.is_available() else “cpu”) print(f”Using device: {device}”)</p> <p>if torch.cuda.device_count() &gt; 1: print(f”Using {torch.cuda.device_count()} GPUs”) model = nn.DataParallel(model) # This will use all available GPUs</p> <p>model = model.to(device)</p> <h1 id="mixup-and-cutmix">Mixup and CutMix</h1> <p>mixup_fn = Mixup( mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None, prob=0.5, # Reduced probability to allow some original images switch_prob=0.5, mode=’batch’, label_smoothing=0.1, num_classes=num_classes )</p> <h1 id="loss-optimizer-and-scheduler">Loss, Optimizer, and Scheduler</h1> <p>criterion = SoftTargetCrossEntropy() # For Mixup and CutMix</p> <h1 id="using-sgd-with-momentum-for-better-fine-tuning">Using SGD with momentum for better fine-tuning</h1> <p>optimizer = optim.SGD( filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=0.9, weight_decay=weight_decay )</p> <h1 id="scheduler-adjusted-to-steps-per-epoch">Scheduler adjusted to steps per epoch</h1> <p>scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)</p> <h1 id="initialize-amp-scaler-for-mixed-precision">Initialize AMP scaler for mixed precision</h1> <p>scaler = torch.amp.GradScaler(device=’cuda’) # Updated instantiation</p> <h1 id="training-and-validation-loop">Training and Validation Loop</h1> <p>writer = SummaryWriter() # For TensorBoard logging</p> <p>def train_one_epoch(epoch): model.train() running_loss, correct, total = 0.0, 0, 0</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Progress bar for training loop
train_loader_tqdm = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Training]", leave=False)
for batch_idx, (images, labels) in enumerate(train_loader_tqdm):
    images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

    # Apply Mixup/CutMix
    images, labels = mixup_fn(images, labels)

    optimizer.zero_grad()

    with torch.cuda.amp.autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

    running_loss += loss.item() * images.size(0)
    total += labels.size(0)

    # Since labels are soft, calculate accuracy based on predicted class vs hard labels
    _, predicted = outputs.max(1)
    _, targets = labels.max(1)
    correct += predicted.eq(targets).sum().item()

    # Update progress bar (accuracy in percentage)
    if (batch_idx + 1) % log_interval == 0 or (batch_idx + 1) == len(train_loader):
        current_loss = loss.item()
        current_acc = 100. * correct / total
        train_loader_tqdm.set_postfix(loss=f"{current_loss:.4f}", accuracy=f"{current_acc:.2f}%")

epoch_loss = running_loss / total
epoch_acc = 100. * correct / total  # Multiply by 100 to get percentage
writer.add_scalar('Loss/train', epoch_loss, epoch)
writer.add_scalar('Accuracy/train', epoch_acc, epoch)
print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%")  # Acc in %
</code></pre></div></div> <p>def validate(epoch): model.eval() val_loss, correct, total = 0.0, 0, 0</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Progress bar for validation loop
val_loader_tqdm = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation]", leave=False)

criterion_val = nn.CrossEntropyLoss()  # Standard loss for validation

with torch.no_grad():
    for batch_idx, (images, labels) in enumerate(val_loader_tqdm):
        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

        with torch.cuda.amp.autocast():
            outputs = model(images)
            loss = criterion_val(outputs, labels)

        val_loss += loss.item() * images.size(0)
        total += labels.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()

        # Update progress bar (accuracy in percentage)
        if (batch_idx + 1) % log_interval == 0 or (batch_idx + 1) == len(val_loader):
            current_loss = loss.item()
            current_acc = 100. * correct / total
            val_loader_tqdm.set_postfix(loss=f"{current_loss:.4f}", accuracy=f"{current_acc:.2f}%")

epoch_loss = val_loss / total
epoch_acc = 100. * correct / total  # Multiply by 100 to get percentage
writer.add_scalar('Loss/val', epoch_loss, epoch)
writer.add_scalar('Accuracy/val', epoch_acc, epoch)
print(f"Validation Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%")  # Acc in %

return epoch_acc
</code></pre></div></div> <h1 id="main-training-loop">Main Training Loop</h1> <p>best_acc = 0 for epoch in range(num_epochs): train_one_epoch(epoch) val_acc = validate(epoch)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Scheduler step
scheduler.step()

# Save best model
if val_acc &gt; best_acc:
    best_acc = val_acc
    os.makedirs('./models', exist_ok=True)
    # If using DataParallel, save the underlying model
    if isinstance(model, nn.DataParallel):
        torch.save(model.module.state_dict(), './models/best_vit_tiny_imagenet.pth')
    else:
        torch.save(model.state_dict(), './models/best_vit_tiny_imagenet.pth')
    print(f"New best model saved with accuracy: {best_acc:.2f}%")
</code></pre></div></div> <p>print(“Training complete. Best validation accuracy:”, best_acc) writer.close()</p>]]></content><author><name></name></author><category term="LLM"/><category term="fine-tune"/><summary type="html"><![CDATA[In this post, I'll generally introduce how to fine-tune a ViT model on a tiny ImageNet dataset.]]></summary></entry><entry><title type="html">分类任务的评价指标以及与MIA的关系</title><link href="https://rhincodone.github.io/posts/2024-11-1-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E4%BB%A5%E5%8F%8A%E4%B8%8EMIA%E7%9A%84%E5%85%B3%E7%B3%BB/" rel="alternate" type="text/html" title="分类任务的评价指标以及与MIA的关系"/><published>2024-11-01T00:00:00+00:00</published><updated>2024-11-01T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E4%BB%A5%E5%8F%8A%E4%B8%8EMIA%E7%9A%84%E5%85%B3%E7%B3%BB</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-11-1-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E4%BB%A5%E5%8F%8A%E4%B8%8EMIA%E7%9A%84%E5%85%B3%E7%B3%BB/"><![CDATA[<h1 id="机器学习模型评估指标详解">机器学习模型评估指标详解</h1> <p>以下是机器学习模型常用评估指标的详细说明，包括<strong>准确率</strong>、<strong>精确率</strong>、<strong>召回率</strong>、<strong>F1 值</strong>、<strong>AUC</strong> 和 <strong>G-mean</strong> 的定义、优缺点。</p> <hr/> <h3 id="1-准确率-accuracy">1. 准确率 (Accuracy)</h3> <ul> <li> <p><strong>定义</strong>：准确率是模型预测正确的样本占所有样本的比例。 \(\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\) 其中，TP 为真正例，TN 为真负例，FP 为假正例，FN 为假负例。</p> </li> <li><strong>优点</strong>：简单直观，适用于类别分布均衡的数据集。</li> <li><strong>缺点</strong>：对于类别不平衡的数据集效果较差，因为少数类的错误可能被多数类的正确预测掩盖。</li> </ul> <hr/> <h3 id="2-精确率-precision">2. 精确率 (Precision)</h3> <ul> <li> <p><strong>定义</strong>：精确率表示在预测为正类的样本中，真正为正类的比例。 \(\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\)</p> </li> <li><strong>优点</strong>：在对误报容忍度较低的应用中（如癌症检测或欺诈检测）尤为重要。</li> <li><strong>缺点</strong>：当负类样本非常多而正类样本很少时，精确率可能较高，导致对召回率的忽视。</li> </ul> <hr/> <h3 id="3-召回率-recall">3. 召回率 (Recall)</h3> <ul> <li> <p><strong>定义</strong>：召回率表示所有实际为正类的样本中被正确预测为正类的比例。 \(\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\)</p> </li> <li><strong>优点</strong>：在对漏报敏感的应用（如疾病检测）中效果显著。</li> <li><strong>缺点</strong>：当模型倾向于将样本预测为正类时，召回率可能较高，但精确率会下降。</li> </ul> <hr/> <h3 id="4-f1-值-f1-score">4. F1 值 (F1 Score)</h3> <ul> <li> <p><strong>定义</strong>：F1 值是精确率和召回率的调和平均值，用于综合评估模型的精确度和召回度。 \(\text{F1 Score} = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\)</p> </li> <li><strong>优点</strong>：适用于类别不平衡的数据集，因为它权衡了精确率和召回率。</li> <li><strong>缺点</strong>：不适用于只关注精确率或召回率的应用场景，因为它无法提供各个指标的详细信息。</li> </ul> <hr/> <h3 id="5-auc-area-under-curve">5. AUC (Area Under Curve)</h3> <ul> <li> <p><strong>定义</strong>：AUC 表示 ROC 曲线下的面积，ROC 曲线是通过绘制不同阈值下的 TPR（真正率）和 FPR（假正率）得到的曲线。 \(\text{AUC} = \int \text{ROC Curve}\)</p> </li> <li><strong>优点</strong>：不受类别分布影响，能够直观衡量模型的分类能力。AUC 越接近 1，模型效果越好。</li> <li><strong>缺点</strong>：对于类别不平衡严重的情况，AUC 可能掩盖少数类的错误分类问题。</li> </ul> <hr/> <h3 id="6-g-mean-几何平均">6. G-Mean (几何平均)</h3> <ul> <li> <p><strong>定义</strong>：G-mean 是模型在正类和负类上的分类效果的几何平均值，通常用于处理类别不平衡的数据。 \(\text{G-Mean} = \sqrt{\text{Recall}_{\text{positive}} \times \text{Recall}_{\text{negative}}}\)</p> </li> <li><strong>优点</strong>：能够平衡模型在不同类别上的表现，适用于类别不平衡的情况。</li> <li><strong>缺点</strong>：可能在极端不平衡数据上表现不佳，因为少数类的表现过于依赖召回率。</li> </ul> <hr/> <h3 id="总结">总结</h3> <table> <thead> <tr> <th>指标</th> <th>定义</th> <th>优点</th> <th>缺点</th> </tr> </thead> <tbody> <tr> <td>准确率</td> <td>所有正确预测的样本占所有样本的比例</td> <td>直观简单，适用于类别平衡的数据集</td> <td>类别不平衡时效果差</td> </tr> <tr> <td>精确率</td> <td>预测为正类的样本中真正为正类的比例</td> <td>适用于误报容忍度低的场景</td> <td>容易忽视召回率</td> </tr> <tr> <td>召回率</td> <td>实际为正类的样本中被正确预测为正类的比例</td> <td>适用于漏报敏感的场景</td> <td>容易忽视精确率</td> </tr> <tr> <td>F1 值</td> <td>精确率和召回率的调和平均值</td> <td>平衡精确率和召回率，适用于类别不平衡的数据</td> <td>无法提供精确率和召回率的详细信息</td> </tr> <tr> <td>AUC</td> <td>ROC 曲线下面积，表示模型对不同阈值下的整体表现</td> <td>不受类别分布影响，能够直观衡量模型分类能力</td> <td>严重不平衡数据时可能掩盖少数类错误</td> </tr> <tr> <td>G-Mean</td> <td>正类和负类召回率的几何平均值，适用于类别不平衡数据</td> <td>平衡模型在不同类别上的表现</td> <td>在极端不平衡数据上表现可能不佳</td> </tr> </tbody> </table> <hr/> <h2 id="tprfprtnrfnrl2-error-和-l1-error">TPR、FPR、TNR、FNR、L2 Error 和 L1 Error</h2> <h3 id="1-tpr-true-positive-rate">1. TPR (True Positive Rate)</h3> <ul> <li><strong>定义</strong>：TPR（真正例率）是所有实际为正类的样本中被正确预测为正类的比例，等同于<strong>召回率</strong>。 \(\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\)</li> </ul> <h3 id="2-fpr-false-positive-rate">2. FPR (False Positive Rate)</h3> <ul> <li><strong>定义</strong>：FPR（假正例率）是所有实际为负类的样本中被错误预测为正类的比例。 \(\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\)</li> </ul> <h3 id="3-tnr-true-negative-rate">3. TNR (True Negative Rate)</h3> <ul> <li><strong>定义</strong>：TNR（真负例率）是所有实际为负类的样本中被正确预测为负类的比例，也称<strong>特异性</strong>。 \(\text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\)</li> </ul> <h3 id="4-fnr-false-negative-rate">4. FNR (False Negative Rate)</h3> <ul> <li><strong>定义</strong>：FNR（假负例率）是所有实际为正类的样本中被错误预测为负类的比例。 \(\text{FNR} = \frac{\text{FN}}{\text{FN} + \text{TP}}\)</li> </ul> <h3 id="5-l2-error-mean-squared-error-mse">5. L2 Error (Mean Squared Error, MSE)</h3> <ul> <li><strong>定义</strong>：L2误差是预测值与实际值之间差值的平方和的均值。 \(\text{L2 Error} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\)</li> </ul> <h3 id="6-l1-error-mean-absolute-error-mae">6. L1 Error (Mean Absolute Error, MAE)</h3> <ul> <li><strong>定义</strong>：L1误差是预测值与实际值之间差值的绝对值的均值。 \(\text{L1 Error} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\)</li> </ul> <hr/> <h2 id="为什么在-mia-中更关注低-fpr-下的-tpr">为什么在 MIA 中更关注低 FPR 下的 TPR？</h2> <p>在<strong>成员推断攻击</strong>（Membership Inference Attack, MIA）中，更关注在低 FPR（False Positive Rate）下的 TPR（True Positive Rate）。原因包括：</p> <ol> <li><strong>隐私风险</strong>：低 FPR 能降低误判非成员为成员的可能性。</li> <li><strong>误报影响</strong>：高 FPR 会导致大量误报，掩盖攻击模型的真实效果。</li> <li><strong>攻击强度</strong>：在极少误报情况下，高 TPR 表示攻击模型在严格条件下的攻击强度。</li> <li><strong>防御敏感性</strong>：低 FPR 下的 TPR 能更好地评估防御机制的有效性。</li> <li><strong>实际需求</strong>：在金融或医疗领域，低 FPR 的高 TPR 更符合实际隐私需求。</li> </ol> <p>低 FPR 下的 TPR 能帮助揭示模型在严格隐私保护条件下的风险。</p>]]></content><author><name></name></author><category term="MIA"/><category term="metrics"/><summary type="html"><![CDATA[本文主要介绍了分类任务的评价标准及优缺点和适用范围，以及MIA attack与各类指标的关系。]]></summary></entry><entry><title type="html">Membership Inferernce Attacks on LLM (Large Language Models)</title><link href="https://rhincodone.github.io/posts/2024-03-23-Machine-Unlearning-On-LLM/" rel="alternate" type="text/html" title="Membership Inferernce Attacks on LLM (Large Language Models)"/><published>2024-03-23T00:00:00+00:00</published><updated>2024-03-23T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/Machine-Unlearning-On-LLM</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-03-23-Machine-Unlearning-On-LLM/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Membership Inference Attacks (MIAs) are designed to determine if a specific data record was used in a model’s training set. Introduced by Shokri et al. in 2017[1], MIAs have since been recognized for their relevance to privacy concerns within machine learning. This introduction will cover the fundamental concept of MIAs and their initial application. Following that, we’ll explore the adaptation of MIAs to large-scale Large Language Models (LLMs) and identify the challenges that diminish MIAs’ effectiveness in this context. To conclude, the discussion will shift to prospective developments and the role MIAs may play in enhancing the privacy of LLMs.</p> <h2 id="membership-inference-attacks">Membership inference attacks</h2> <p>The intuition behind MIAs lies in overfitting of the model to the training data. Overfitting is a common challenge in machine learning, where a model learns too much from its training data, capturing excessive detail that hinders its ability to generalize to unseen data. This occurs when a model, after extensive training, performs exceptionally well on the training data (for instance, achieving 100% accuracy) but poorly on a testing dataset, effectively making random guesses. The root of this issue lies in the model memorizing specific details of the training images—such as the exact pixel positions—rather than learning the broader, general features necessary to distinguish between different individuals’ faces. Although overfitting cannot be completely eliminated due to the model’s inherent limitation of learning solely from its training dataset, various strategies can mitigate its effects.</p> <p>This concept of overfitting also underpins the differential performance of models on training versus non-training samples, evident through their loss function outputs. For example, a model might show lower loss (e.g., smaller cross-entropy) for training samples than for testing samples. This discrepancy is exploited in membership inference attacks, which aim to determine whether a specific sample was part of the model’s training set based on the model’s response to that sample.</p> <p>In early implementations of membership inference attacks, the approach involves splitting an original dataset into two parts: one for members (data included in the training set) and one for non-members (data excluded from the training set). The “member” data is used to train a target model, resulting in a model trained specifically on that subset. Subsequently, samples from both the member and non-member datasets are input into the trained model, which outputs a probability vector for each sample if the model is of the classification type. These vectors, along with labels indicating “member” or “non-member” status based on the sample’s origin, are combined with the sample’s true classification label to form a new dataset. This new dataset, referred to as the attack dataset, includes each sample’s probability vector, its true class label, and its membership label. An attack model—a binary classification model—is then trained on this attack dataset to discern whether a given sample was part of the model’s training set.</p> <h2 id="mia-on-llm">MIA on LLM</h2> <p>Although MIA has been deeply investigated in small-scale networks, in the era of large languge model (LLM), it is not studied on the LLM enough. Most of the works directly adopt the intuition of MIA-classify the members and non-members based on some indicators (The implementations of the attacks can be found at [6]):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MIALLM-480.webp 480w,/assets/img/MIALLM-800.webp 800w,/assets/img/MIALLM-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/MIALLM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p><strong>LOSS [2]:</strong> Considers the model’s computed loss over the target sample. \begin{equation}f(x;M) = L(x;M)\end{equation}</p> </li> <li> <p><strong>Reference-based [3]:</strong> Attempts to improve the precision of the LOSS attack and reduce the false negative rate by accounting for the intrinsic complexity of the target point \(x\) by calibrating \(L(x;M)\), with respect to another reference model \((M_{ref})\), which is trained on data from the same distribution as \(D\), but not necessarily the same data. \begin{equation}f(x;M) = L(x;M) - L(x;M_{ref})\end{equation}</p> </li> <li> <p><strong>Zlib Entropy [3]:</strong> Calibrates the sample’s loss under the target model using the sample’s zlib compression size. \begin{equation}f(x;M) = \frac{L(x;M)}{zlib(x)}\end{equation}</p> </li> <li> <p><strong>Min-k% Prob [4]:</strong> Uses the \(k\%\) of tokens with the lowest likelihoods to compute a score instead of averaging over all token probabilities as in loss. \begin{equation}f(x;M) = \frac{1}{|min-k(x)|} \sum_{x_i \in min-k(x)} -\log(p(x_i | x_1, …, x_{i-1}))\end{equation}</p> </li> <li> <p><strong>Neighborhood Attack [5]:</strong> Uses an estimate of the curvature of the loss function at a given sample, computed by perturbing the target sequence to create \(n\) ‘neighboring’ points, and comparing the loss of the target \(x\), with its neighbors \(\tilde{x}\). \begin{equation}f(x;M) = L(x;M) - \frac{1}{n} \sum_{i=1}^{n} L(\tilde{x}_i;M)\end{equation}</p> </li> </ol> <h2 id="why-mia-cares">Why MIA cares</h2> <p>Membership Inference Attacks (MIA) are significant for privacy because they can expose whether an individual’s data was used in training a machine learning model. This might seem technical but has real-world privacy implications, especially in sensitive contexts. For a simple example, consider a machine learning model trained on health records to predict disease outcomes. If an attacker can use MIA to determine that a particular individual’s data was used in the training set, they might indirectly learn something about that individual’s health status, even if the data was supposed to be anonymous.</p> <p>Imagine a hospital that develops a model to predict diabetes risk based on patient records. If an attacker can apply MIA to this model and discover that John Doe’s data was used in training, they could infer that John might have been at risk of or diagnosed with diabetes, information that’s supposed to be private. This scenario illustrates why MIA is a concern for privacy: it can lead to unintended disclosures of personal information, undermining the anonymity and confidentiality of sensitive data.</p> <h2 id="problems-in-mia">Problems in MIA</h2> <p>As research into Membership Inference Attacks (MIA) progresses, several complexities and challenges have emerged. A key question is the interpretation of the probabilities produced by MIA models. For instance, if an MIA model assigns a 60% probability to a sample being a “member” (i.e., part of the training data), the implications for privacy remain unclear. This uncertainty extends to minor modifications of the sample, such as altering a single pixel—does this still warrant a “member” classification, and what does that mean for privacy?</p> <p>Carlini et al. have highlighted that MIA tends to be more effective at identifying “non-member” records rather than “members.”[7] This observation suggests that the privacy risks associated with membership inference might not be as significant for member samples as previously thought.</p> <p>Furthermore, the nuances of Large Language Models (LLMs) and their susceptibility to MIAs warrant further exploration. Specific characteristics of LLMs, such as their capacity for data memorization, play a crucial role in how they might compromise the anonymity of training data. However, the full extent of these features’ impact on training data membership remains partially understood.</p> <p>Given these issues, the future and practical relevance of MIA are subjects of ongoing debate. As we continue to unravel the complexities of MIAs and their effects on privacy, it is crucial to refine our understanding of these attacks and their implications for the security of machine learning models.</p> <h2 id="potential-of-mia">Potential of MIA</h2> <p>Recent research highlights the potential of Membership Inference Attacks (MIA) for auditing the privacy of algorithms designed for differential privacy. Differential privacy provides a solid guarantee of privacy, leading to the proposal of differentially private machine learning algorithms, such as DP-SGD and DP-Adam. Users set a theoretical privacy budget, \(\epsilon_{theory}\), and a relaxation parameter, \(\delta\), aiming for the trained model to achieve \((\epsilon_{theory},\delta)\)-differential privacy. However, assessing a model’s actual privacy level has been challenging, as differential privacy offers a worst-case guarantee, suggesting that the practical privacy level might be more stringent (practical \(\epsilon\) is smaller than \(\epsilon_{theory}\)).</p> <p>Studies now indicate that MIA can estimate the lower bound of the practical \(\epsilon\), denoted as \(\epsilon_{LB}\). This suggests the actual \(\epsilon\) for a model’s privacy lies within the range \([\epsilon_{LB},\epsilon_{theory}]\), providing a clearer picture of its privacy assurances. This insight underscores MIA’s role in evaluating and ensuring the privacy of differentially private machine learning algorithms[8][9][10].</p> <p>Several differentially private fine-tuning techniques have been developed for Large Language Models (LLMs) to secure their differential privacy [11][12]. Despite these advancements, there remains a lack of research into the auditing of these models’ actual privacy levels. Therefore, a valuable direction for future work could be to employ Membership Inference Attacks (MIAs) to assess and audit the privacy of LLMs that have undergone differential privacy fine-tuning. This approach could provide a clearer understanding of how private the LLMs really are, beyond theoretical guarantees.</p> <h2 id="references">References</h2> <p>[1] Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017, May). Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP) (pp. 3-18). IEEE.</p> <p>[2] Yeom, S., Giacomelli, I., Fredrikson, M., &amp; Jha, S. (2018, July). Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF) (pp. 268-282). IEEE.</p> <p>[3] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … &amp; Raffel, C. (2021). Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) (pp. 2633-2650).</p> <p>[4] Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., … &amp; Zettlemoyer, L. (2023). Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789.</p> <p>[5] Mattern, J., Mireshghallah, F., Jin, Z., Schölkopf, B., Sachan, M., &amp; Berg-Kirkpatrick, T. (2023). Membership inference attacks against language models via neighbourhood comparison. arXiv preprint arXiv:2305.18462.</p> <p>[6] https://github.com/iamgroot42/mimir</p> <p>[7] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis and F. Tramèr, “Membership Inference Attacks From First Principles,” 2022 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 2022, pp. 1897-1914, doi: 10.1109/SP46214.2022.9833649.</p> <p>[8] Tramer, F., Terzis, A., Steinke, T., Song, S., Jagielski, M., &amp; Carlini, N. (2022). Debugging differential privacy: A case study for privacy auditing. arXiv preprint arXiv:2202.12219.</p> <p>[9] Nasr, M., Hayes, J., Steinke, T., Balle, B., Tramèr, F., Jagielski, M., … &amp; Terzis, A. (2023). Tight auditing of differentially private machine learning. In 32nd USENIX Security Symposium (USENIX Security 23) (pp. 1631-1648).</p> <p>[10] Steinke, T., Nasr, M., &amp; Jagielski, M. (2024). Privacy auditing with one (1) training run. Advances in Neural Information Processing Systems, 36.</p> <p>[11] Behnia, R., Ebrahimi, M. R., Pacheco, J., &amp; Padmanabhan, B. (2022, November). Ew-tune: A framework for privately fine-tuning large language models with differential privacy. In 2022 IEEE International Conference on Data Mining Workshops (ICDMW) (pp. 560-566). IEEE.</p> <p>[12] Singh, T., Aditya, H., Madisetti, V. K., &amp; Bahga, A. (2024). Whispered Tuning: Data Privacy Preservation in Fine-Tuning LLMs through Differential Privacy. Journal of Software Engineering and Applications, 17(1), 1-22.</p>]]></content><author><name></name></author><category term="Machine"/><category term="Unlearning"/><category term="MU"/><summary type="html"><![CDATA[In this post, I'll introduce what is membership inference attack, how it's used on LLM, and what's the potential problem on it.]]></summary></entry><entry><title type="html">A Foundation of Differential Privacy</title><link href="https://rhincodone.github.io/posts/2024-03-05-Differential-Privacy/" rel="alternate" type="text/html" title="A Foundation of Differential Privacy"/><published>2024-03-05T00:00:00+00:00</published><updated>2024-03-05T00:00:00+00:00</updated><id>https://rhincodone.github.io/posts/Differential-Privacy</id><content type="html" xml:base="https://rhincodone.github.io/posts/2024-03-05-Differential-Privacy/"><![CDATA[<h2 id="introduction-with-an-example">Introduction with an example</h2> <p>Differential privacy (DP) is a privacy-preserving concept in data analysis and statistics. The goal of differential privacy is to allow the inclusion of an individual’s data in a dataset for analysis while protecting the privacy of that individual.</p> <h2 id="example">Example</h2> <p>So, how does DP achieve this? Let’s take a simple example:</p> <p>Suppose you are part of a company that wants to calculate the average salary of its employees without revealing the salary of any specific employee. The company collects the following data:</p> <p><em>Employee A: $60,000</em> <em>Employee B: $70,000</em> <em>Employee C: $80,000</em> <em>Employee D: $90,000</em></p> <p><strong>Non-Differentially Private Calculation:</strong></p> <p>In a traditional average calculation, you might sum all salaries and divide by the number of employees: \begin{equation} \text{Average} = \frac{60,000 + 70,000 + 80,000 + 90,000}{4} = 75,000 \end{equation}</p> <p><strong>Differentially Private Calculation:</strong></p> <p>In a differentially private approach, you add random noise to the calculation to protect individual privacy. Let’s say you add random noise between -1,000 and +1,000 to each employee’s salary:</p> <p>\begin{equation} \text{Average} = \frac{60,000 + 70,000 + 80,000 + 90,000 + \text{noise}}{4} \end{equation}</p> <p>Here, the noise is a random value chosen from the range -1,000 to +1,000. This ensures that even if an individual’s salary changes slightly, it is challenging to determine which specific employee’s salary contributed to the final result.</p> <p>In essence, differential privacy aims to introduce uncertainty in the output distribution. Considering two datasets that differ only in one data record, a differentially private algorithm will generate output distributions that are slightly different for these two datasets. In other words, observers cannot discern which dataset the algorithm used from the output, and thus, they cannot determine whether a specific data record is in the dataset or not.</p>]]></content><author><name></name></author><category term="Differential"/><category term="privacy"/><category term="DP"/><summary type="html"><![CDATA[In this post, I'll generally introduce what is differential privacy with an example]]></summary></entry></feed>