<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fine-Tuning Vision Transformer (ViT) on Tiny ImageNet Dataset | Yuechun (Ethan) Gu </title> <meta name="author" content="Yuechun (Ethan) Gu"> <meta name="description" content="In this post, I'll generally introduce how to fine-tune a ViT model on a tiny ImageNet dataset."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rhincodone.github.io/posts/2024-11-15-Fine_Tuning_ViT_Tiny_ImageNet/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuechun (Ethan)</span> Gu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">BLOG </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">PUBLICATIONS </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">SERVICES </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-Tuning Vision Transformer (ViT) on Tiny ImageNet Dataset</h1> <p class="post-meta"> November 15, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> fine-tune     ·   <i class="fa-solid fa-tag fa-sm"></i> LLM   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>This document provides a detailed overview of the strategy employed to fine-tune a Vision Transformer (ViT) on the Tiny ImageNet dataset, achieving a validation accuracy of <strong>90.5% within 10 epochs</strong>.</p> <h2 id="dataset-description">Dataset Description</h2> <ul> <li> <strong>Dataset</strong>: Tiny ImageNet</li> <li> <strong>Number of Classes</strong>: 200</li> <li> <strong>Image Size</strong>: 64x64 resized to 384x384 for ViT</li> </ul> <h2 id="model-configuration">Model Configuration</h2> <ul> <li> <strong>Model</strong>: ViT-Base with patch size 16 (<code class="language-plaintext highlighter-rouge">vit_base_patch16_384</code>)</li> <li> <strong>Pretrained Weights</strong>: Used pretrained weights from ImageNet</li> <li> <strong>Optimizer</strong>: SGD with momentum (0.9)</li> <li> <strong>Learning Rate</strong>: 1e-4</li> <li> <strong>Weight Decay</strong>: 0.01</li> <li> <strong>Scheduler</strong>: Cosine Annealing Learning Rate</li> <li> <strong>Loss Function</strong>: Soft Target Cross-Entropy (for Mixup/CutMix)</li> <li> <strong>Augmentation</strong>: RandAugment, Random Erasing, Mixup, and CutMix</li> </ul> <h2 id="strategy">Strategy</h2> <h3 id="data-preprocessing">Data Preprocessing</h3> <ol> <li> <strong>Image Resizing</strong>: <ul> <li>Images were resized to 384x384 to match the input dimensions required by the Vision Transformer (ViT) model. This ensures that the patching mechanism of the ViT (16x16 patches in this case) works seamlessly, dividing the images into the correct number of patches for transformer-based processing.</li> </ul> </li> <li> <strong>Enhanced Data Augmentations</strong>: <ul> <li> <strong>RandAugment</strong>: <ul> <li>Method: This augmentation policy applies a random combination of transformations such as rotation, brightness adjustment, and flipping, chosen from a predefined pool of operations.</li> <li>Implementation: Integrated using the <code class="language-plaintext highlighter-rouge">RandAugment</code> class from <code class="language-plaintext highlighter-rouge">torchvision.transforms</code>.</li> <li>Intuition: Augmentations simulate diverse scenarios in the dataset, enhancing model robustness to unseen variations in real-world applications.</li> </ul> </li> <li> <strong>Random Erasing</strong>: <ul> <li>Method: Randomly erases parts of an image during training by replacing selected regions with random pixel values.</li> <li>Probability: Set to 0.25, meaning 25% of training images had a random region erased.</li> <li>Intuition: Prevents the model from over-relying on specific regions of an image, encouraging it to learn more generalized features.</li> </ul> </li> </ul> </li> </ol> <h3 id="training-enhancements">Training Enhancements</h3> <ol> <li> <strong>Mixup and CutMix</strong>: <ul> <li> <strong>Mixup</strong>: <ul> <li>Method: Mixup blends two training examples and their labels, creating a synthetic training sample:<br> [ \tilde{x} = \lambda x_i + (1 - \lambda) x_j, \quad \tilde{y} = \lambda y_i + (1 - \lambda) y_j ]<br> where ( \lambda ) is sampled from a Beta distribution.</li> <li>Implementation: Integrated using the <code class="language-plaintext highlighter-rouge">Mixup</code> utility from the <code class="language-plaintext highlighter-rouge">timm</code> library.</li> <li>Intuition: Mixup smoothens decision boundaries and reduces overfitting, as the model cannot rely on “hard” training labels.</li> </ul> </li> <li> <strong>CutMix</strong>: <ul> <li>Method: Similar to Mixup, but instead of blending the entire images, rectangular patches of one image replace patches in another. Labels are proportionally adjusted.</li> <li>Implementation: Configured with probabilities for blending and patch placement using <code class="language-plaintext highlighter-rouge">timm.data.Mixup</code>.</li> <li>Intuition: Encourages spatially aware feature learning, improving robustness to occlusions or corruptions.</li> </ul> </li> </ul> </li> <li> <strong>Stochastic Depth</strong>: <ul> <li>Method: During training, randomly drops a subset of transformer blocks in each forward pass, controlled by a drop probability.</li> <li>Implementation: Applied a drop probability of 0.1 to regularize deeper layers using <code class="language-plaintext highlighter-rouge">timm.layers.DropPath</code>.</li> <li>Intuition: Mimics an ensemble effect by allowing the model to explore multiple sub-networks, reducing overfitting and improving generalization.</li> </ul> </li> <li> <strong>AMP (Automatic Mixed Precision)</strong>: <ul> <li>Method: Combines half-precision and full-precision computations dynamically during training.</li> <li>Implementation: Enabled with <code class="language-plaintext highlighter-rouge">torch.amp.GradScaler</code> and <code class="language-plaintext highlighter-rouge">torch.cuda.amp.autocast</code>.</li> <li>Intuition: Reduces GPU memory usage and accelerates training while maintaining model performance, especially useful for computationally intensive ViT models.</li> </ul> </li> </ol> <h3 id="training-loop">Training Loop</h3> <ul> <li> <strong>Epochs</strong>: Trained for up to 50 epochs but utilized early stopping after achieving peak validation accuracy (90.5%) at 10 epochs.</li> <li> <strong>Batch Size</strong>: Set to 128, optimized for GPU memory utilization.</li> <li> <strong>Logging</strong>: Metrics, including training and validation loss and accuracy, were logged using TensorBoard. Logging frequency was every 100 batches to balance granularity and performance overhead.</li> </ul> <h3 id="validation">Validation</h3> <ul> <li>Standard Cross-Entropy loss was used during validation for hard-label accuracy computation. Unlike the training phase, which used soft-label losses (Mixup and CutMix), validation focused purely on the model’s ability to classify with confidence in real-world scenarios.</li> </ul> <hr> <h3 id="layer-fine-tuning-strategy">Layer Fine-Tuning Strategy</h3> <p>The experiment tested two configurations for fine-tuning:</p> <ol> <li> <strong>Fine-Tuning All Layers</strong>: <ul> <li>In this setting, all layers of the ViT model were unfrozen, allowing gradient updates to modify the pretrained weights.</li> <li> <strong>Result</strong>: Achieved a validation accuracy of 90.5%, demonstrating the ability of the model to adapt its internal representations to the Tiny ImageNet dataset.</li> </ul> </li> <li> <strong>Fine-Tuning the Last Fully Connected Layer Only</strong>: <ul> <li>In this setting, only the final classification head (Fully Connected Layer) was updated, while all transformer layers were frozen.</li> <li> <strong>Result</strong>: Achieved a validation accuracy of 72.3%, indicating limited capacity to adapt the learned features to the new dataset.</li> </ul> </li> </ol> <p><strong>Analysis</strong>:</p> <ul> <li> <strong>Why Fine-Tuning All Layers Performed Better</strong>: <ul> <li>The pretrained ViT model was trained on ImageNet, which shares some similarities with Tiny ImageNet but differs in scale and distribution.</li> <li>Fine-tuning all layers allowed the model to adjust its intermediate representations to the specific features and patterns of the Tiny ImageNet dataset, leading to significantly better performance.</li> </ul> </li> <li> <strong>When to Fine-Tune Specific Layers</strong>: <ul> <li>Fine-tuning specific layers, such as only the classification head, may suffice for tasks with highly similar datasets (e.g., same domain). However, for diverse datasets, fine-tuning more or all layers is generally necessary.</li> </ul> </li> </ul> <hr> <p><strong>Key Takeaway</strong>: Fine-tuning the entire network maximized the model’s adaptability to Tiny ImageNet, yielding superior performance. However, this comes at a higher computational cost compared to only tuning the last layer.</p> <h2 id="results">Results</h2> <ul> <li> <strong>Validation Accuracy</strong>: 90.5% after 10 epochs</li> <li> <strong>Training Time</strong>: Approximately 30 minutes per epoch on a single GPU</li> <li> <strong>Best Model Saved</strong>: Model checkpoint saved at <code class="language-plaintext highlighter-rouge">./models/best_vit_tiny_imagenet.pth</code> </li> </ul> <h2 id="key-insights">Key Insights</h2> <ol> <li> <strong>Enhanced Augmentations</strong>: The combination of RandAugment, Mixup, and CutMix improved generalization.</li> <li> <strong>Cosine Annealing</strong>: Helped achieve smooth convergence with the learning rate.</li> <li> <strong>Pretrained Weights</strong>: Accelerated convergence and boosted performance significantly.</li> </ol> <hr> <p><strong>Repository Setup</strong>: The code for this implementation, including the preprocessing and training pipeline, is structured for easy reproducibility. Ensure you have the following dependencies installed:</p> <ul> <li>PyTorch</li> <li>torchvision</li> <li>timm</li> <li>tqdm</li> </ul> <h2 id="code">Code</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">shutil</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">timm</span> <span class="kn">import</span> <span class="n">create_model</span>
<span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingLR</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>  <span class="c1"># For progress bar
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">HF_HOME</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/tmp/ygu2/hf_cache_custom</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">HUGGINGFACE_HUB_CACHE</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/tmp/ygu2/hf_cache_custom/hub</span><span class="sh">'</span>

<span class="c1"># Import Mixup and CutMix utilities from timm
</span><span class="kn">from</span> <span class="n">timm.data</span> <span class="kn">import</span> <span class="n">Mixup</span>
<span class="kn">from</span> <span class="n">timm.loss</span> <span class="kn">import</span> <span class="n">SoftTargetCrossEntropy</span>

<span class="c1"># Optional: Import RandAugment for enhanced data augmentation
</span><span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">RandAugment</span>

<span class="c1"># Set CuDNN Benchmark for optimized performance
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># Paths and Constants
</span><span class="n">data_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./datasets/tiny-imagenet-200</span><span class="sh">"</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># Adjust based on GPU memory
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># Increased number of epochs for better convergence
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>  <span class="c1"># Lowered learning rate for fine-tuning
</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># Adjusted weight decay
</span><span class="n">image_size</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">log_interval</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Log metrics every 100 batches
</span>
<span class="c1"># Reorganize validation data
</span><span class="n">val_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">)</span>
<span class="n">val_images_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">val_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">images</span><span class="sh">'</span><span class="p">)</span>
<span class="n">val_annotations_file</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">val_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">val_annotations.txt</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Create a mapping from image filenames to their labels
</span><span class="n">val_img_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">val_annotations_file</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">():</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="se">\t</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">val_img_dict</span><span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create directories for each class if they don't exist
</span><span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">set</span><span class="p">(</span><span class="n">val_img_dict</span><span class="p">.</span><span class="nf">values</span><span class="p">()):</span>
    <span class="n">label_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">val_images_dir</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">label_dir</span><span class="p">):</span>
        <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">label_dir</span><span class="p">)</span>

<span class="c1"># Move images into the corresponding label directories
</span><span class="k">for</span> <span class="n">img_filename</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">val_img_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">val_images_dir</span><span class="p">,</span> <span class="n">img_filename</span><span class="p">)</span>
    <span class="n">dst</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">val_images_dir</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">img_filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">src</span><span class="p">):</span>
        <span class="n">shutil</span><span class="p">.</span><span class="nf">move</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>

<span class="c1"># Data Augmentation and Transformations
</span><span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">InterpolationMode</span><span class="p">.</span><span class="n">BICUBIC</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="nc">RandAugment</span><span class="p">(),</span>  <span class="c1"># Enhanced augmentation
</span>    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomErasing</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.25</span><span class="p">),</span>
<span class="p">])</span>

<span class="n">transform_test</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">InterpolationMode</span><span class="p">.</span><span class="n">BICUBIC</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">((</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">)),</span>
<span class="p">])</span>

<span class="c1"># Load Datasets
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_train</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Reduced from 8 to 2
</span>    <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">val_images_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_test</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
    <span class="n">val_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Reduced from 8 to 2
</span>    <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Create Vision Transformer (ViT) Model
</span><span class="n">model</span> <span class="o">=</span> <span class="nf">create_model</span><span class="p">(</span><span class="sh">'</span><span class="s">vit_base_patch16_384</span><span class="sh">'</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># Apply Stochastic Depth
</span><span class="kn">from</span> <span class="n">timm.layers</span> <span class="kn">import</span> <span class="n">DropPath</span>  <span class="c1"># Updated import path
</span>
<span class="k">def</span> <span class="nf">apply_stochastic_depth</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">drop_prob</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">DropPath</span><span class="p">):</span>
            <span class="n">module</span><span class="p">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>

<span class="nf">apply_stochastic_depth</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">drop_prob</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Unfreeze the entire model for fine-tuning
</span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1"># Use DataParallel for multiple GPUs if available
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Using </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">()</span><span class="si">}</span><span class="s"> GPUs</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># This will use all available GPUs
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Mixup and CutMix
</span><span class="n">mixup_fn</span> <span class="o">=</span> <span class="nc">Mixup</span><span class="p">(</span>
    <span class="n">mixup_alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">cutmix_alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">cutmix_minmax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Reduced probability to allow some original images
</span>    <span class="n">switch_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">batch</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span>
<span class="p">)</span>

<span class="c1"># Loss, Optimizer, and Scheduler
</span><span class="n">criterion</span> <span class="o">=</span> <span class="nc">SoftTargetCrossEntropy</span><span class="p">()</span>  <span class="c1"># For Mixup and CutMix
</span>
<span class="c1"># Using SGD with momentum for better fine-tuning
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span>
    <span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span>
<span class="p">)</span>

<span class="c1"># Scheduler adjusted to steps per epoch
</span><span class="n">scheduler</span> <span class="o">=</span> <span class="nc">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="c1"># Initialize AMP scaler for mixed precision
</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nc">GradScaler</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># Updated instantiation
</span>
<span class="c1"># Training and Validation Loop
</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">()</span>  <span class="c1"># For TensorBoard logging
</span>
<span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">running_loss</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="c1"># Progress bar for training loop
</span>    <span class="n">train_loader_tqdm</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s"> [Training]</span><span class="sh">"</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader_tqdm</span><span class="p">):</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Apply Mixup/CutMix
</span>        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nf">mixup_fn</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">scaler</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">scaler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>

        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">images</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Since labels are soft, calculate accuracy based on predicted class vs hard labels
</span>        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

        <span class="c1"># Update progress bar (accuracy in percentage)
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">current_acc</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
            <span class="n">train_loader_tqdm</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="n">total</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>  <span class="c1"># Multiply by 100 to get percentage
</span>    <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss/train</span><span class="sh">'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy/train</span><span class="sh">'</span><span class="p">,</span> <span class="n">epoch_acc</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s">], Loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Acc: </span><span class="si">{</span><span class="n">epoch_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Acc in %
</span>
<span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">val_loss</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="c1"># Progress bar for validation loop
</span>    <span class="n">val_loader_tqdm</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s"> [Validation]</span><span class="sh">"</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">criterion_val</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># Standard loss for validation
</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">val_loader_tqdm</span><span class="p">):</span>
            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">():</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion_val</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

            <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">images</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

            <span class="c1"># Update progress bar (accuracy in percentage)
</span>            <span class="nf">if </span><span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">):</span>
                <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">current_acc</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
                <span class="n">val_loader_tqdm</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">current_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">val_loss</span> <span class="o">/</span> <span class="n">total</span>
    <span class="n">epoch_acc</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>  <span class="c1"># Multiply by 100 to get percentage
</span>    <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss/val</span><span class="sh">'</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy/val</span><span class="sh">'</span><span class="p">,</span> <span class="n">epoch_acc</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Validation Loss: </span><span class="si">{</span><span class="n">epoch_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Acc: </span><span class="si">{</span><span class="n">epoch_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Acc in %
</span>
    <span class="k">return</span> <span class="n">epoch_acc</span>

<span class="c1"># Main Training Loop
</span><span class="n">best_acc</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="nf">validate</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

    <span class="c1"># Scheduler step
</span>    <span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1"># Save best model
</span>    <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
        <span class="n">best_acc</span> <span class="o">=</span> <span class="n">val_acc</span>
        <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">./models</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># If using DataParallel, save the underlying model
</span>        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">):</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">'</span><span class="s">./models/best_vit_tiny_imagenet.pth</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">'</span><span class="s">./models/best_vit_tiny_imagenet.pth</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">New best model saved with accuracy: </span><span class="si">{</span><span class="n">best_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>print(“Training complete. Best validation accuracy:”, best_acc) writer.close()</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yuechun (Ethan) Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>