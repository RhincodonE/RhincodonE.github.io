<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fine-Tuning Vision Transformer (ViT) on Tiny ImageNet Dataset | Yuechun (Ethan) Gu </title> <meta name="author" content="Yuechun (Ethan) Gu"> <meta name="description" content="In this post, I'll generally introduce how to fine-tune a ViT model on a tiny ImageNet dataset."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rhincodone.github.io/posts/2024-11-15-Fine_Tuning_ViT_Tiny_ImageNet/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuechun (Ethan)</span> Gu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">BLOG </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">PUBLICATIONS </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">SERVICES </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-Tuning Vision Transformer (ViT) on Tiny ImageNet Dataset</h1> <p class="post-meta"> November 15, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> fine-tune     ·   <i class="fa-solid fa-tag fa-sm"></i> LLM   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>This document provides a detailed overview of the strategy employed to fine-tune a Vision Transformer (ViT) on the Tiny ImageNet dataset, achieving a validation accuracy of <strong>90.5% within 10 epochs</strong>.</p> <h2 id="dataset-description">Dataset Description</h2> <ul> <li> <strong>Dataset</strong>: Tiny ImageNet</li> <li> <strong>Number of Classes</strong>: 200</li> <li> <strong>Image Size</strong>: 64x64 resized to 384x384 for ViT</li> </ul> <h2 id="model-configuration">Model Configuration</h2> <ul> <li> <strong>Model</strong>: ViT-Base with patch size 16 (<code class="language-plaintext highlighter-rouge">vit_base_patch16_384</code>)</li> <li> <strong>Pretrained Weights</strong>: Used pretrained weights from ImageNet</li> <li> <strong>Optimizer</strong>: SGD with momentum (0.9)</li> <li> <strong>Learning Rate</strong>: 1e-4</li> <li> <strong>Weight Decay</strong>: 0.01</li> <li> <strong>Scheduler</strong>: Cosine Annealing Learning Rate</li> <li> <strong>Loss Function</strong>: Soft Target Cross-Entropy (for Mixup/CutMix)</li> <li> <strong>Augmentation</strong>: RandAugment, Random Erasing, Mixup, and CutMix</li> </ul> <h2 id="strategy">Strategy</h2> <h3 id="data-preprocessing">Data Preprocessing</h3> <ol> <li> <strong>Image Resizing</strong>: <ul> <li>Images were resized to 384x384 to match the input dimensions required by the Vision Transformer (ViT) model. This ensures that the patching mechanism of the ViT (16x16 patches in this case) works seamlessly, dividing the images into the correct number of patches for transformer-based processing.</li> </ul> </li> <li> <strong>Enhanced Data Augmentations</strong>: <ul> <li> <strong>RandAugment</strong>: <ul> <li>Method: This augmentation policy applies a random combination of transformations such as rotation, brightness adjustment, and flipping, chosen from a predefined pool of operations.</li> <li>Implementation: Integrated using the <code class="language-plaintext highlighter-rouge">RandAugment</code> class from <code class="language-plaintext highlighter-rouge">torchvision.transforms</code>.</li> <li>Intuition: Augmentations simulate diverse scenarios in the dataset, enhancing model robustness to unseen variations in real-world applications.</li> </ul> </li> <li> <strong>Random Erasing</strong>: <ul> <li>Method: Randomly erases parts of an image during training by replacing selected regions with random pixel values.</li> <li>Probability: Set to 0.25, meaning 25% of training images had a random region erased.</li> <li>Intuition: Prevents the model from over-relying on specific regions of an image, encouraging it to learn more generalized features.</li> </ul> </li> </ul> </li> </ol> <h3 id="training-enhancements">Training Enhancements</h3> <ol> <li> <strong>Mixup and CutMix</strong>: <ul> <li> <strong>Mixup</strong>: <ul> <li>Method: Mixup blends two training examples and their labels, creating a synthetic training sample:<br> [ \tilde{x} = \lambda x_i + (1 - \lambda) x_j, \quad \tilde{y} = \lambda y_i + (1 - \lambda) y_j ]<br> where ( \lambda ) is sampled from a Beta distribution.</li> <li>Implementation: Integrated using the <code class="language-plaintext highlighter-rouge">Mixup</code> utility from the <code class="language-plaintext highlighter-rouge">timm</code> library.</li> <li>Intuition: Mixup smoothens decision boundaries and reduces overfitting, as the model cannot rely on “hard” training labels.</li> </ul> </li> <li> <strong>CutMix</strong>: <ul> <li>Method: Similar to Mixup, but instead of blending the entire images, rectangular patches of one image replace patches in another. Labels are proportionally adjusted.</li> <li>Implementation: Configured with probabilities for blending and patch placement using <code class="language-plaintext highlighter-rouge">timm.data.Mixup</code>.</li> <li>Intuition: Encourages spatially aware feature learning, improving robustness to occlusions or corruptions.</li> </ul> </li> </ul> </li> <li> <strong>Stochastic Depth</strong>: <ul> <li>Method: During training, randomly drops a subset of transformer blocks in each forward pass, controlled by a drop probability.</li> <li>Implementation: Applied a drop probability of 0.1 to regularize deeper layers using <code class="language-plaintext highlighter-rouge">timm.layers.DropPath</code>.</li> <li>Intuition: Mimics an ensemble effect by allowing the model to explore multiple sub-networks, reducing overfitting and improving generalization.</li> </ul> </li> <li> <strong>AMP (Automatic Mixed Precision)</strong>: <ul> <li>Method: Combines half-precision and full-precision computations dynamically during training.</li> <li>Implementation: Enabled with <code class="language-plaintext highlighter-rouge">torch.amp.GradScaler</code> and <code class="language-plaintext highlighter-rouge">torch.cuda.amp.autocast</code>.</li> <li>Intuition: Reduces GPU memory usage and accelerates training while maintaining model performance, especially useful for computationally intensive ViT models.</li> </ul> </li> </ol> <h3 id="training-loop">Training Loop</h3> <ul> <li> <strong>Epochs</strong>: Trained for up to 50 epochs but utilized early stopping after achieving peak validation accuracy (90.5%) at 10 epochs.</li> <li> <strong>Batch Size</strong>: Set to 128, optimized for GPU memory utilization.</li> <li> <strong>Logging</strong>: Metrics, including training and validation loss and accuracy, were logged using TensorBoard. Logging frequency was every 100 batches to balance granularity and performance overhead.</li> </ul> <h3 id="validation">Validation</h3> <ul> <li>Standard Cross-Entropy loss was used during validation for hard-label accuracy computation. Unlike the training phase, which used soft-label losses (Mixup and CutMix), validation focused purely on the model’s ability to classify with confidence in real-world scenarios.</li> </ul> <hr> <h3 id="layer-fine-tuning-strategy">Layer Fine-Tuning Strategy</h3> <p>The experiment tested two configurations for fine-tuning:</p> <ol> <li> <strong>Fine-Tuning All Layers</strong>: <ul> <li>In this setting, all layers of the ViT model were unfrozen, allowing gradient updates to modify the pretrained weights.</li> <li> <strong>Result</strong>: Achieved a validation accuracy of 90.5%, demonstrating the ability of the model to adapt its internal representations to the Tiny ImageNet dataset.</li> </ul> </li> <li> <strong>Fine-Tuning the Last Fully Connected Layer Only</strong>: <ul> <li>In this setting, only the final classification head (Fully Connected Layer) was updated, while all transformer layers were frozen.</li> <li> <strong>Result</strong>: Achieved a validation accuracy of 72.3%, indicating limited capacity to adapt the learned features to the new dataset.</li> </ul> </li> </ol> <p><strong>Analysis</strong>:</p> <ul> <li> <strong>Why Fine-Tuning All Layers Performed Better</strong>: <ul> <li>The pretrained ViT model was trained on ImageNet, which shares some similarities with Tiny ImageNet but differs in scale and distribution.</li> <li>Fine-tuning all layers allowed the model to adjust its intermediate representations to the specific features and patterns of the Tiny ImageNet dataset, leading to significantly better performance.</li> </ul> </li> <li> <strong>When to Fine-Tune Specific Layers</strong>: <ul> <li>Fine-tuning specific layers, such as only the classification head, may suffice for tasks with highly similar datasets (e.g., same domain). However, for diverse datasets, fine-tuning more or all layers is generally necessary.</li> </ul> </li> </ul> <hr> <p><strong>Key Takeaway</strong>: Fine-tuning the entire network maximized the model’s adaptability to Tiny ImageNet, yielding superior performance. However, this comes at a higher computational cost compared to only tuning the last layer.</p> <h2 id="results">Results</h2> <ul> <li> <strong>Validation Accuracy</strong>: 90.5% after 10 epochs</li> <li> <strong>Training Time</strong>: Approximately 30 minutes per epoch on a single GPU</li> <li> <strong>Best Model Saved</strong>: Model checkpoint saved at <code class="language-plaintext highlighter-rouge">./models/best_vit_tiny_imagenet.pth</code> </li> </ul> <h2 id="key-insights">Key Insights</h2> <ol> <li> <strong>Enhanced Augmentations</strong>: The combination of RandAugment, Mixup, and CutMix improved generalization.</li> <li> <strong>Cosine Annealing</strong>: Helped achieve smooth convergence with the learning rate.</li> <li> <strong>Pretrained Weights</strong>: Accelerated convergence and boosted performance significantly.</li> </ol> <hr> <p><strong>Repository Setup</strong>: The code for this implementation, including the preprocessing and training pipeline, is structured for easy reproducibility. Ensure you have the following dependencies installed:</p> <ul> <li>PyTorch</li> <li>torchvision</li> <li>timm</li> <li>tqdm</li> </ul> <h2 id="code">Code</h2> <p>````markdown ```python</p> <p>import os import shutil import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from timm import create_model from torch.optim.lr_scheduler import CosineAnnealingLR from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter from tqdm import tqdm # For progress bar os.environ[‘HF_HOME’] = ‘/tmp/ygu2/hf_cache_custom’ os.environ[‘HUGGINGFACE_HUB_CACHE’] = ‘/tmp/ygu2/hf_cache_custom/hub’</p> <h1 id="import-mixup-and-cutmix-utilities-from-timm">Import Mixup and CutMix utilities from timm</h1> <p>from timm.data import Mixup from timm.loss import SoftTargetCrossEntropy</p> <h1 id="optional-import-randaugment-for-enhanced-data-augmentation">Optional: Import RandAugment for enhanced data augmentation</h1> <p>from torchvision.transforms import RandAugment</p> <h1 id="set-cudnn-benchmark-for-optimized-performance">Set CuDNN Benchmark for optimized performance</h1> <p>torch.backends.cudnn.benchmark = True</p> <h1 id="paths-and-constants">Paths and Constants</h1> <p>data_dir = “./datasets/tiny-imagenet-200” num_classes = 200 batch_size = 128 # Adjust based on GPU memory num_epochs = 50 # Increased number of epochs for better convergence learning_rate = 1e-4 # Lowered learning rate for fine-tuning weight_decay = 0.01 # Adjusted weight decay image_size = 384 log_interval = 100 # Log metrics every 100 batches</p> <h1 id="reorganize-validation-data">Reorganize validation data</h1> <p>val_dir = os.path.join(data_dir, ‘val’) val_images_dir = os.path.join(val_dir, ‘images’) val_annotations_file = os.path.join(val_dir, ‘val_annotations.txt’)</p> <h1 id="create-a-mapping-from-image-filenames-to-their-labels">Create a mapping from image filenames to their labels</h1> <p>val_img_dict = {} with open(val_annotations_file, ‘r’) as f: for line in f.readlines(): words = line.strip().split(‘\t’) val_img_dict[words[0]] = words[1]</p> <h1 id="create-directories-for-each-class-if-they-dont-exist">Create directories for each class if they don’t exist</h1> <p>for label in set(val_img_dict.values()): label_dir = os.path.join(val_images_dir, label) if not os.path.exists(label_dir): os.makedirs(label_dir)</p> <h1 id="move-images-into-the-corresponding-label-directories">Move images into the corresponding label directories</h1> <p>for img_filename, label in val_img_dict.items(): src = os.path.join(val_images_dir, img_filename) dst = os.path.join(val_images_dir, label, img_filename) if os.path.exists(src): shutil.move(src, dst)</p> <h1 id="data-augmentation-and-transformations">Data Augmentation and Transformations</h1> <p>transform_train = transforms.Compose([ transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC), transforms.RandomHorizontalFlip(), RandAugment(), # Enhanced augmentation transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), transforms.RandomErasing(p=0.25), ])</p> <p>transform_test = transforms.Compose([ transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), ])</p> <h1 id="load-datasets">Load Datasets</h1> <p>train_dataset = datasets.ImageFolder(os.path.join(data_dir, ‘train’), transform=transform_train) train_loader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, # Reduced from 8 to 2 pin_memory=True, prefetch_factor=4, persistent_workers=True )</p> <p>val_dataset = datasets.ImageFolder(val_images_dir, transform=transform_test) val_loader = DataLoader( val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, # Reduced from 8 to 2 pin_memory=True, prefetch_factor=4, persistent_workers=True )</p> <h1 id="create-vision-transformer-vit-model">Create Vision Transformer (ViT) Model</h1> <p>model = create_model(‘vit_base_patch16_384’, pretrained=True, num_classes=num_classes)</p> <h1 id="apply-stochastic-depth">Apply Stochastic Depth</h1> <p>from timm.layers import DropPath # Updated import path</p> <p>def apply_stochastic_depth(model, drop_prob): for module in model.modules(): if isinstance(module, DropPath): module.drop_prob = drop_prob</p> <p>apply_stochastic_depth(model, drop_prob=0.1)</p> <h1 id="unfreeze-the-entire-model-for-fine-tuning">Unfreeze the entire model for fine-tuning</h1> <p>for param in model.parameters(): param.requires_grad = True</p> <h1 id="use-dataparallel-for-multiple-gpus-if-available">Use DataParallel for multiple GPUs if available</h1> <p>device = torch.device(“cuda” if torch.cuda.is_available() else “cpu”) print(f”Using device: {device}”)</p> <p>if torch.cuda.device_count() &gt; 1: print(f”Using {torch.cuda.device_count()} GPUs”) model = nn.DataParallel(model) # This will use all available GPUs</p> <p>model = model.to(device)</p> <h1 id="mixup-and-cutmix">Mixup and CutMix</h1> <p>mixup_fn = Mixup( mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None, prob=0.5, # Reduced probability to allow some original images switch_prob=0.5, mode=’batch’, label_smoothing=0.1, num_classes=num_classes )</p> <h1 id="loss-optimizer-and-scheduler">Loss, Optimizer, and Scheduler</h1> <p>criterion = SoftTargetCrossEntropy() # For Mixup and CutMix</p> <h1 id="using-sgd-with-momentum-for-better-fine-tuning">Using SGD with momentum for better fine-tuning</h1> <p>optimizer = optim.SGD( filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=0.9, weight_decay=weight_decay )</p> <h1 id="scheduler-adjusted-to-steps-per-epoch">Scheduler adjusted to steps per epoch</h1> <p>scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)</p> <h1 id="initialize-amp-scaler-for-mixed-precision">Initialize AMP scaler for mixed precision</h1> <p>scaler = torch.amp.GradScaler(device=’cuda’) # Updated instantiation</p> <h1 id="training-and-validation-loop">Training and Validation Loop</h1> <p>writer = SummaryWriter() # For TensorBoard logging</p> <p>def train_one_epoch(epoch): model.train() running_loss, correct, total = 0.0, 0, 0</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Progress bar for training loop
train_loader_tqdm = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Training]", leave=False)
for batch_idx, (images, labels) in enumerate(train_loader_tqdm):
    images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

    # Apply Mixup/CutMix
    images, labels = mixup_fn(images, labels)

    optimizer.zero_grad()

    with torch.cuda.amp.autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

    running_loss += loss.item() * images.size(0)
    total += labels.size(0)

    # Since labels are soft, calculate accuracy based on predicted class vs hard labels
    _, predicted = outputs.max(1)
    _, targets = labels.max(1)
    correct += predicted.eq(targets).sum().item()

    # Update progress bar (accuracy in percentage)
    if (batch_idx + 1) % log_interval == 0 or (batch_idx + 1) == len(train_loader):
        current_loss = loss.item()
        current_acc = 100. * correct / total
        train_loader_tqdm.set_postfix(loss=f"{current_loss:.4f}", accuracy=f"{current_acc:.2f}%")

epoch_loss = running_loss / total
epoch_acc = 100. * correct / total  # Multiply by 100 to get percentage
writer.add_scalar('Loss/train', epoch_loss, epoch)
writer.add_scalar('Accuracy/train', epoch_acc, epoch)
print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%")  # Acc in %
</code></pre></div></div> <p>def validate(epoch): model.eval() val_loss, correct, total = 0.0, 0, 0</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Progress bar for validation loop
val_loader_tqdm = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation]", leave=False)

criterion_val = nn.CrossEntropyLoss()  # Standard loss for validation

with torch.no_grad():
    for batch_idx, (images, labels) in enumerate(val_loader_tqdm):
        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)

        with torch.cuda.amp.autocast():
            outputs = model(images)
            loss = criterion_val(outputs, labels)

        val_loss += loss.item() * images.size(0)
        total += labels.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()

        # Update progress bar (accuracy in percentage)
        if (batch_idx + 1) % log_interval == 0 or (batch_idx + 1) == len(val_loader):
            current_loss = loss.item()
            current_acc = 100. * correct / total
            val_loader_tqdm.set_postfix(loss=f"{current_loss:.4f}", accuracy=f"{current_acc:.2f}%")

epoch_loss = val_loss / total
epoch_acc = 100. * correct / total  # Multiply by 100 to get percentage
writer.add_scalar('Loss/val', epoch_loss, epoch)
writer.add_scalar('Accuracy/val', epoch_acc, epoch)
print(f"Validation Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%")  # Acc in %

return epoch_acc
</code></pre></div></div> <h1 id="main-training-loop">Main Training Loop</h1> <p>best_acc = 0 for epoch in range(num_epochs): train_one_epoch(epoch) val_acc = validate(epoch)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Scheduler step
scheduler.step()

# Save best model
if val_acc &gt; best_acc:
    best_acc = val_acc
    os.makedirs('./models', exist_ok=True)
    # If using DataParallel, save the underlying model
    if isinstance(model, nn.DataParallel):
        torch.save(model.module.state_dict(), './models/best_vit_tiny_imagenet.pth')
    else:
        torch.save(model.state_dict(), './models/best_vit_tiny_imagenet.pth')
    print(f"New best model saved with accuracy: {best_acc:.2f}%")
</code></pre></div></div> <p>print(“Training complete. Best validation accuracy:”, best_acc)</p> <p>writer.close()</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yuechun (Ethan) Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>